{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "459a3b5f",
   "metadata": {},
   "source": [
    "# 03 - Extract per-frame embeddings (EfficientNet-B3 backbone)\n",
    "This notebook produces embeddings/<split>/<video_stem>.npy for each video (shape: T x feat_dim).\n",
    "It will use your local spatial checkpoint if you provide its path; otherwise it will use a timm pretrained EfficientNet-B3 backbone.\n",
    "Run cell-by-cell. This is GPU-accelerated and resumable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca81cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "# ------------- USER CONFIG -------------\n",
    "ROOT = Path.cwd()\n",
    "FRAMES_ROOT = ROOT / \"preprocessed\" / \"frames\"   # <split>/<video_stem>/frame_00.jpg ...\n",
    "EMB_ROOT = ROOT / \"embeddings\"                  # outputs go here\n",
    "SPLITS = [\"train\", \"val\", \"test\"]\n",
    "BATCH_FRAMES = 16      # how many frames to run through backbone at once (GPU memory)\n",
    "IMG_SIZE = 224\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SPATIAL_CKPT = None    # if you have a local checkpoint file path, set it (string) else keep None\n",
    "# Example: SPATIAL_CKPT = \"/home/me/checkpoints/spatial/spatial_best_valAUC.pth\"\n",
    "USE_TIMM_PRETRAINED = True   # if SPATIAL_CKPT is None, use timm pretrained weights\n",
    "# ---------------------------------------\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "print(\"Frames root:\", FRAMES_ROOT)\n",
    "print(\"Embeddings root:\", EMB_ROOT)\n",
    "EMB_ROOT.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643c3f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Keep transforms simple and deterministic - same normalization used for training typically\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),    # gives [C,H,W], float in [0,1]\n",
    "    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225))\n",
    "])\n",
    "\n",
    "def load_frame_tensor(p: Path):\n",
    "    img = Image.open(str(p)).convert(\"RGB\")\n",
    "    return preprocess(img)  # tensor [3,H,W]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4997039",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureBackbone(nn.Module):\n",
    "    def __init__(self, model_name=\"efficientnet_b3\", use_pretrained=True):\n",
    "        super().__init__()\n",
    "        # create backbone with no classifier head (num_classes=0 gives feature vector)\n",
    "        self.backbone = timm.create_model(model_name, pretrained=use_pretrained, num_classes=0)\n",
    "        self.feat_dim = self.backbone.num_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B,3,H,W] -> returns [B, feat_dim]\n",
    "        return self.backbone(x)\n",
    "\n",
    "# instantiate\n",
    "print(\"Loading backbone...\")\n",
    "if SPATIAL_CKPT:\n",
    "    # When user supplies a checkpoint, we still create the model with pretrained=False then load weights\n",
    "    backbone = FeatureBackbone(use_pretrained=False)\n",
    "    # load checkpoint tolerant to different keys\n",
    "    ckpt = torch.load(SPATIAL_CKPT, map_location=\"cpu\")\n",
    "    if \"model_state_dict\" in ckpt:\n",
    "        state = ckpt[\"model_state_dict\"]\n",
    "    elif \"state_dict\" in ckpt:\n",
    "        state = ckpt[\"state_dict\"]\n",
    "    else:\n",
    "        state = ckpt\n",
    "    # strip \"module.\" prefix if present\n",
    "    new_state = {}\n",
    "    for k,v in state.items():\n",
    "        nk = k.replace(\"module.\", \"\") if k.startswith(\"module.\") else k\n",
    "        # If checkpoint had a head, ignore head keys (we only expect backbone feature extractor)\n",
    "        # try to map keys: keep those that match our model\n",
    "        new_state[nk] = v\n",
    "    missing, unexpected = backbone.backbone.load_state_dict(new_state, strict=False)\n",
    "    print(\"Loaded checkpoint:\", SPATIAL_CKPT)\n",
    "    print(\"Missing keys:\", missing)\n",
    "    print(\"Unexpected keys:\", unexpected)\n",
    "else:\n",
    "    backbone = FeatureBackbone(use_pretrained=USE_TIMM_PRETRAINED)\n",
    "    print(\"Using timm pretrained EfficientNet-B3.\")\n",
    "\n",
    "backbone = backbone.to(DEVICE)\n",
    "backbone.eval()\n",
    "feat_dim = backbone.feat_dim\n",
    "print(\"Backbone feature dim:\", feat_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91301885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_for_video(video_stem: str, split: str):\n",
    "    frames_dir = FRAMES_ROOT / split / video_stem\n",
    "    if not frames_dir.exists():\n",
    "        return {\"status\":\"missing_frames_folder\"}\n",
    "    out_path = EMB_ROOT / split / f\"{video_stem}.npy\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    # resume if already exists\n",
    "    if out_path.exists():\n",
    "        return {\"status\":\"exists\"}\n",
    "\n",
    "    # collect frame files sorted\n",
    "    frame_files = sorted([p for p in frames_dir.glob(\"frame_*.jpg\")])\n",
    "    if len(frame_files) == 0:\n",
    "        return {\"status\":\"no_frames_found\"}\n",
    "\n",
    "    # load all frames tensors into a list\n",
    "    tensors = [load_frame_tensor(p) for p in frame_files]  # list of [3,H,W]\n",
    "    # batch through backbone in B x 3 x H x W\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(tensors), BATCH_FRAMES):\n",
    "            batch = torch.stack(tensors[i:i+BATCH_FRAMES], dim=0).to(DEVICE)  # [b,3,H,W]\n",
    "            with autocast(enabled=(DEVICE.type==\"cuda\")):\n",
    "                feats = backbone(batch)  # [b,feat_dim]\n",
    "            feats = feats.detach().cpu().numpy()\n",
    "            embeddings.append(feats)\n",
    "    embeddings = np.vstack(embeddings)  # shape (T, feat_dim)\n",
    "    np.save(str(out_path), embeddings)\n",
    "    return {\"status\":\"saved\", \"shape\": embeddings.shape, \"path\": str(out_path)}\n",
    "\n",
    "# run per split\n",
    "summary = {}\n",
    "for split in SPLITS:\n",
    "    split_dir = FRAMES_ROOT / split\n",
    "    if not split_dir.exists():\n",
    "        print(f\"Split frames folder not found: {split_dir} (skipping)\")\n",
    "        continue\n",
    "    stems = sorted([p.name for p in split_dir.iterdir() if p.is_dir()])\n",
    "    print(f\"Processing split {split} - videos found: {len(stems)}\")\n",
    "    summary[split] = {\"total\": len(stems), \"saved\":0, \"exists\":0, \"missing\":0}\n",
    "    for stem in tqdm(stems):\n",
    "        res = extract_embeddings_for_video(stem, split)\n",
    "        if res[\"status\"] == \"saved\":\n",
    "            summary[split][\"saved\"] += 1\n",
    "        elif res[\"status\"] == \"exists\":\n",
    "            summary[split][\"exists\"] += 1\n",
    "        else:\n",
    "            summary[split][\"missing\"] += 1\n",
    "\n",
    "# write manifest\n",
    "EMB_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "with open(EMB_ROOT / \"manifest.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"Done. Summary:\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28befcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect one saved embedding to make sure shapes are right\n",
    "for split in SPLITS:\n",
    "    pdir = EMB_ROOT / split\n",
    "    if pdir.exists():\n",
    "        files = sorted(list(pdir.glob(\"*.npy\")))\n",
    "        if files:\n",
    "            arr = np.load(files[0])\n",
    "            print(\"Sample:\", files[0].name, \"shape:\", arr.shape, \"dtype:\", arr.dtype)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e2c498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
