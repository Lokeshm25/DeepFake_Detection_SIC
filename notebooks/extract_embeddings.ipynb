{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "459a3b5f",
   "metadata": {},
   "source": [
    "# Extract embeddings from trained spatial model\n",
    "Saves embeddings/<split>/<video_stem>.npy (shape T x feat_dim).\n",
    "Be sure the checkpoint path below points to your trained checkpoint (spatial_best_valAUC.pth).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ca81cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Checkpoint path: c:\\Users\\lkmah\\OneDrive\\Desktop\\Lokesh\\VS Code\\DeepFake_Detection_SIC\\checkpoints\\spatial\\spatial_best_valAUC.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json, os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch import nn\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# CONFIGURATION\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "FRAMES_ROOT = ROOT / \"preprocessed\" / \"frames\"\n",
    "EMB_ROOT = ROOT / \"embeddings\"\n",
    "CHECKPOINT_DIR = ROOT / \"checkpoints\" / \"spatial\"\n",
    "CHECKPOINT_PATH = CHECKPOINT_DIR / \"spatial_best_valAUC.pth\"   # change if using spatial_last.pth\n",
    "SPLITS = [\"train\", \"val\", \"test\"]\n",
    "IMG_SIZE = 224\n",
    "BATCH = 16                # frames per batch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EMB_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "print(\"Checkpoint path:\", CHECKPOINT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "643c3f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model. Backbone feat dim: 1536\n"
     ]
    }
   ],
   "source": [
    "# define model architecture (same as during training)\n",
    "class SpatialModel(nn.Module):\n",
    "    def __init__(self, backbone_name=\"efficientnet_b3\", pretrained=False, head_hidden=512, dropout=0.4):\n",
    "        super().__init__()\n",
    "        # backbone as during training: num_classes=0 to output features\n",
    "        self.backbone = timm.create_model(backbone_name, pretrained=pretrained, num_classes=0)\n",
    "        feat_dim = self.backbone.num_features\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(feat_dim, head_hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(head_hidden, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)\n",
    "        logits = self.head(feats).squeeze(1)\n",
    "        return logits\n",
    "\n",
    "# instantiate model and load checkpoint\n",
    "model = SpatialModel(pretrained=False).to(DEVICE)\n",
    "if not CHECKPOINT_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Checkpoint not found: {CHECKPOINT_PATH}\")\n",
    "\n",
    "ck = torch.load(CHECKPOINT_PATH, map_location=\"cpu\")\n",
    "# ck may have keys like 'model_state' depending on saver; handle both forms\n",
    "state = ck.get(\"model_state\", ck)\n",
    "# If state dict has 'module.' prefixes or similar, load strict=False\n",
    "try:\n",
    "    model.load_state_dict(state, strict=False)\n",
    "except Exception as e:\n",
    "    # try to strip 'module.' prefix if present\n",
    "    new_state = {}\n",
    "    for k,v in state.items():\n",
    "        nk = k.replace(\"module.\", \"\") if k.startswith(\"module.\") else k\n",
    "        new_state[nk] = v\n",
    "    model.load_state_dict(new_state, strict=False)\n",
    "\n",
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "\n",
    "FEAT_DIM = model.backbone.num_features\n",
    "print(\"Loaded model. Backbone feat dim:\", FEAT_DIM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4997039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define image preprocessing (same as during training)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "])\n",
    "\n",
    "def load_frame_tensor(p: Path):\n",
    "    img = Image.open(str(p)).convert(\"RGB\")\n",
    "    t = preprocess(img)\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91301885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function to extract and save embeddings for a single video\n",
    "def extract_for_video(split, video_stem, batch_size=BATCH):\n",
    "    frames_dir = FRAMES_ROOT / split / video_stem\n",
    "    if not frames_dir.exists():\n",
    "        return {\"status\":\"missing_frames\"}\n",
    "    out_path = EMB_ROOT / split / f\"{video_stem}.npy\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if out_path.exists():\n",
    "        return {\"status\":\"exists\"}\n",
    "\n",
    "    frame_files = sorted(list(frames_dir.glob(\"frame_*.jpg\")))\n",
    "    if len(frame_files) == 0:\n",
    "        return {\"status\":\"no_frames\"}\n",
    "\n",
    "    # create tensors list\n",
    "    tensors = [load_frame_tensor(p) for p in frame_files]\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(tensors), batch_size):\n",
    "            batch = torch.stack(tensors[i:i+batch_size], dim=0).to(DEVICE)\n",
    "            # use autocast (mixed precision) for speed if CUDA available\n",
    "            with autocast(enabled=(DEVICE.type==\"cuda\")):\n",
    "                feats = model.backbone(batch)   # [b, feat_dim]\n",
    "            \n",
    "            embeddings.append(feats.detach().cpu().float().numpy())\n",
    "    embeddings = np.vstack(embeddings)  # (T, feat_dim)\n",
    "    np.save(str(out_path), embeddings)\n",
    "    return {\"status\":\"saved\", \"shape\": embeddings.shape}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d28befcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: split=train videos=4066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4066 [00:00<?, ?it/s]C:\\Users\\lkmah\\AppData\\Local\\Temp\\ipykernel_11220\\3087726621.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE.type==\"cuda\")):\n",
      "100%|██████████| 4066/4066 [01:42<00:00, 39.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train done: {'total': 4066, 'saved': 4046, 'exists': 20, 'missing': 0, 'no_frames': 0}\n",
      "Extracting embeddings: split=val videos=762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 762/762 [00:20<00:00, 37.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val done: {'total': 762, 'saved': 741, 'exists': 20, 'missing': 0, 'no_frames': 1}\n",
      "Extracting embeddings: split=test videos=255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 255/255 [00:06<00:00, 40.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test done: {'total': 255, 'saved': 235, 'exists': 20, 'missing': 0, 'no_frames': 0}\n",
      "Wrote embeddings manifest: c:\\Users\\lkmah\\OneDrive\\Desktop\\Lokesh\\VS Code\\DeepFake_Detection_SIC\\embeddings\\manifest.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "manifest = {}\n",
    "# smoke test toggle: set to True to test a few videos only\n",
    "SMOKE_TEST = False\n",
    "SMOKE_COUNT = 20\n",
    "\n",
    "for split in SPLITS:\n",
    "    split_dir = FRAMES_ROOT / split\n",
    "    if not split_dir.exists():\n",
    "        print(\"Skipping missing split:\", split)\n",
    "        continue\n",
    "    stems = sorted([p.name for p in split_dir.iterdir() if p.is_dir()])\n",
    "    if SMOKE_TEST:\n",
    "        stems = stems[:SMOKE_COUNT]\n",
    "    print(f\"Extracting embeddings: split={split} videos={len(stems)}\")\n",
    "    manifest[split] = {\"total\": len(stems), \"saved\":0, \"exists\":0, \"missing\":0, \"no_frames\":0}\n",
    "    for stem in tqdm(stems):\n",
    "        res = extract_for_video(split, stem, batch_size=BATCH)\n",
    "        s = res[\"status\"]\n",
    "        if s == \"saved\":\n",
    "            manifest[split][\"saved\"] += 1\n",
    "        elif s == \"exists\":\n",
    "            manifest[split][\"exists\"] += 1\n",
    "        elif s == \"missing_frames\":\n",
    "            manifest[split][\"missing\"] += 1\n",
    "        elif s == \"no_frames\":\n",
    "            manifest[split][\"no_frames\"] += 1\n",
    "    print(split, \"done:\", manifest[split])\n",
    "\n",
    "# Save manifest\n",
    "with open(EMB_ROOT / \"manifest.json\", \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(\"Wrote embeddings manifest:\", EMB_ROOT / \"manifest.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17e2c498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 000.npy shape: (8, 1536)\n"
     ]
    }
   ],
   "source": [
    "# inspect one saved embedding\n",
    "for split in SPLITS:\n",
    "    pdir = EMB_ROOT / split\n",
    "    if pdir.exists():\n",
    "        files = sorted(list(pdir.glob(\"*.npy\")))\n",
    "        if files:\n",
    "            arr = np.load(files[0])\n",
    "            print(\"Sample:\", files[0].name, \"shape:\", arr.shape)\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
