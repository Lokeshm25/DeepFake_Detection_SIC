{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c912718a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Model loaded.\n",
      "Deleted old embedding: c:\\Users\\lkmah\\OneDrive\\Desktop\\Lokesh\\VS Code\\DeepFake_Detection_SIC\\embeddings\\train\\837.npy\n",
      "Frames found: 8\n",
      "Saved new embedding: c:\\Users\\lkmah\\OneDrive\\Desktop\\Lokesh\\VS Code\\DeepFake_Detection_SIC\\embeddings\\train\\837.npy\n",
      "Shape: (8, 1536)\n",
      "Verification:\n",
      "any NaN: False\n",
      "any Inf: False\n",
      "min/max/mean: -0.23082626 1244.7639 7.593655\n"
     ]
    }
   ],
   "source": [
    "# reextract_one_embedding.py\n",
    "# Re-extracts a single embedding safely in FP32\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "ROOT = Path.cwd().parent\n",
    "SPLIT = \"train\"\n",
    "VIDEO_STEM = \"837\"   # <<< CHANGE IF NEEDED\n",
    "\n",
    "FRAMES_ROOT = ROOT / \"preprocessed\" / \"frames\"\n",
    "EMB_ROOT = ROOT / \"embeddings\"\n",
    "CHECKPOINT_PATH = ROOT / \"checkpoints\" / \"spatial\" / \"spatial_best_valAUC.pth\"\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH = 16\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# --------------------------------------\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# ---------------- MODEL ----------------\n",
    "class SpatialModel(nn.Module):\n",
    "    def __init__(self, backbone_name=\"efficientnet_b3\", pretrained=False, head_hidden=512, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(backbone_name, pretrained=pretrained, num_classes=0)\n",
    "        feat_dim = self.backbone.num_features\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(feat_dim, head_hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(head_hidden, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)\n",
    "        logits = self.head(feats).squeeze(1)\n",
    "        return logits\n",
    "\n",
    "model = SpatialModel(pretrained=False).to(DEVICE)\n",
    "\n",
    "ck = torch.load(CHECKPOINT_PATH, map_location=\"cpu\")\n",
    "state = ck.get(\"model_state\", ck)\n",
    "\n",
    "# strip \"module.\" if present\n",
    "new_state = {}\n",
    "for k, v in state.items():\n",
    "    nk = k.replace(\"module.\", \"\") if k.startswith(\"module.\") else k\n",
    "    new_state[nk] = v\n",
    "\n",
    "model.load_state_dict(new_state, strict=False)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# ---------------- PREPROCESS ----------------\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "])\n",
    "\n",
    "def load_frame(p):\n",
    "    img = Image.open(p).convert(\"RGB\")\n",
    "    return preprocess(img)\n",
    "\n",
    "# ---------------- RE-EXTRACT ----------------\n",
    "frames_dir = FRAMES_ROOT / SPLIT / VIDEO_STEM\n",
    "out_path = EMB_ROOT / SPLIT / f\"{VIDEO_STEM}.npy\"\n",
    "\n",
    "if not frames_dir.exists():\n",
    "    raise FileNotFoundError(f\"Frames not found: {frames_dir}\")\n",
    "\n",
    "# delete old corrupted file\n",
    "if out_path.exists():\n",
    "    out_path.unlink()\n",
    "    print(\"Deleted old embedding:\", out_path)\n",
    "\n",
    "frame_files = sorted(frames_dir.glob(\"frame_*.jpg\"))\n",
    "print(\"Frames found:\", len(frame_files))\n",
    "\n",
    "tensors = [load_frame(p) for p in frame_files]\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(tensors), BATCH):\n",
    "        batch = torch.stack(tensors[i:i+BATCH]).to(DEVICE)\n",
    "        feats = model.backbone(batch)   # â— FP32 ONLY\n",
    "        embeddings.append(feats.cpu().numpy())\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "np.save(out_path, embeddings)\n",
    "\n",
    "print(\"Saved new embedding:\", out_path)\n",
    "print(\"Shape:\", embeddings.shape)\n",
    "\n",
    "# ---------------- VERIFY ----------------\n",
    "print(\"Verification:\")\n",
    "print(\"any NaN:\", np.isnan(embeddings).any())\n",
    "print(\"any Inf:\", np.isinf(embeddings).any())\n",
    "print(\"min/max/mean:\",\n",
    "      np.min(embeddings),\n",
    "      np.max(embeddings),\n",
    "      np.mean(embeddings))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
