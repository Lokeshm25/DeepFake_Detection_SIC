{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c93cbe48",
   "metadata": {},
   "source": [
    "# 01 - Simple dataset split\n",
    "This notebook creates deterministic train/val/test splits and a labels.json mapping.\n",
    "It expects your preprocessed face-only folders to be present under the project root:\n",
    "- preprocessed/DFDC_REAL_Face_only_data/\n",
    "- preprocessed/DFDC_FAKE_Face_only_data/\n",
    "- preprocessed/FF_Face_only_data/  (optional metadata.csv)\n",
    "- preprocessed/Celeb_real_face_only/  (optional: not used for training)\n",
    "- preprocessed/Celeb_fake_face_only/  (optional: not used for training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffd81d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: c:\\Users\\lkmah\\OneDrive\\Desktop\\Lokesh\\VS Code\\DeepFake_Detection_SIC\n",
      "PREPROC: c:\\Users\\lkmah\\OneDrive\\Desktop\\Lokesh\\VS Code\\DeepFake_Detection_SIC\\Dataset\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import json\n",
    "import yaml\n",
    "import pandas as pd   # used only if ffpp metadata present\n",
    "\n",
    "# --------- USER EDIT ---------\n",
    "ROOT = Path.cwd().parent\n",
    "print(\"ROOT:\", ROOT)\n",
    "PREPROC = ROOT / \"Dataset\"\n",
    "print(\"PREPROC:\", PREPROC) # where your preprocessed face-folders live\n",
    "OUT_DIR = ROOT / \"data\"         # outputs written here\n",
    "RESERVE_COUNT = 200             # internal reserved set\n",
    "SEED = 42                       # deterministic seed\n",
    "TRAIN_FRAC = 0.80\n",
    "VAL_FRAC = 0.15\n",
    "# -----------------------------\n",
    "\n",
    "# dataset folders (update names only if your folders are different)\n",
    "DFDC_REAL = PREPROC / \"DFDC_REAL_Face_only_data\"\n",
    "DFDC_FAKE = PREPROC / \"DFDC_FAKE_Face_only_data\"\n",
    "FFPP_DIR = PREPROC / \"FF_Face_only_data\"          # optional\n",
    "FFPP_META = FFPP_DIR / \"metadata.csv\"             # optional file\n",
    "CELEB_REAL = PREPROC / \"Celeb_real_face_only\"     # optional (held-out)\n",
    "CELEB_FAKE = PREPROC / \"Celeb_fake_face_only\"     # optional (held-out)\n",
    "\n",
    "OUT_DIR.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37d2d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_videos(folder: Path):\n",
    "    \"\"\"Return list of Path objects. If folder contains subfolders per video, prefer those (use subfolder name).\"\"\"\n",
    "    if not folder.exists():\n",
    "        return []\n",
    "    # if subfolders exist, treat each subfolder as one 'video' (preprocessed face folder)\n",
    "    subdirs = [p for p in folder.iterdir() if p.is_dir()]\n",
    "    if len(subdirs) > 0:\n",
    "        # use subdir path as representative; label mapping will use subdir.name as key\n",
    "        return [p for p in subdirs]\n",
    "    # else fallback: list files directly\n",
    "    files = [p for p in folder.glob(\"*\") if p.is_file()]\n",
    "    return files\n",
    "\n",
    "entries = []  # each entry: dict { \"path\": str, \"stem\": str, \"label\": 0/1, \"source\": name }\n",
    "\n",
    "# DFDC real\n",
    "for p in list_videos(DFDC_REAL):\n",
    "    entries.append({\"path\": str(p), \"stem\": p.stem, \"label\": 0, \"source\": \"dfdc_real\"})\n",
    "\n",
    "# DFDC fake\n",
    "for p in list_videos(DFDC_FAKE):\n",
    "    entries.append({\"path\": str(p), \"stem\": p.stem, \"label\": 1, \"source\": \"dfdc_fake\"})\n",
    "\n",
    "# FF++ (try metadata first if present, else infer from parent or filename)\n",
    "ff_meta = {}\n",
    "if FFPP_META.exists():\n",
    "    try:\n",
    "        df = pd.read_csv(FFPP_META)\n",
    "        # try commonly named columns\n",
    "        # prefer columns named 'video' and 'label' (if they exist)\n",
    "        if 'video' in df.columns and 'label' in df.columns:\n",
    "            for _, row in df.iterrows():\n",
    "                ff_meta[str(Path(row['video']).stem)] = int(row['label'])\n",
    "        else:\n",
    "            # build a flexible mapping: find first two meaningful columns\n",
    "            # (simple heuristic)\n",
    "            cols = list(df.columns)\n",
    "            keycol = cols[0]\n",
    "            labcol = None\n",
    "            for c in cols[1:]:\n",
    "                if any(x in c.lower() for x in ['label','fake','class','manipulated']):\n",
    "                    labcol = c\n",
    "                    break\n",
    "            if labcol is None:\n",
    "                labcol = cols[1] if len(cols) > 1 else None\n",
    "            if labcol:\n",
    "                for _, row in df.iterrows():\n",
    "                    ff_meta[str(Path(row[keycol]).stem)] = 1 if str(row[labcol]).strip().lower() in ('1','true','fake','t','y','yes') else 0\n",
    "    except Exception as e:\n",
    "        print(\"Warning: unable to parse FF++ metadata.csv:\", e)\n",
    "\n",
    "# Now list FF++ videos/folders and map labels\n",
    "for p in list_videos(FFPP_DIR):\n",
    "    stem = p.stem\n",
    "    if stem in ff_meta:\n",
    "        label = ff_meta[stem]\n",
    "    else:\n",
    "        # fallback: try to infer from foldername or parent folder\n",
    "        parent = str(p.parent).lower()\n",
    "        if 'fake' in parent:\n",
    "            label = 1\n",
    "        elif 'real' in parent:\n",
    "            label = 0\n",
    "        else:\n",
    "            # if cannot determine label, skip (safer)\n",
    "            print(f\"Skipping FF++ entry (no label): {p}\")\n",
    "            continue\n",
    "    entries.append({\"path\": str(p), \"stem\": stem, \"label\": int(label), \"source\": \"ffpp\"})\n",
    "\n",
    "# Celeb-DF (optional) - we include in entries but you'll not use them for training\n",
    "for p in list_videos(CELEB_REAL):\n",
    "    entries.append({\"path\": str(p), \"stem\": p.stem, \"label\": 0, \"source\": \"celeb_real\"})\n",
    "for p in list_videos(CELEB_FAKE):\n",
    "    entries.append({\"path\": str(p), \"stem\": p.stem, \"label\": 1, \"source\": \"celeb_fake\"})\n",
    "\n",
    "print(\"Collected entries:\", len(entries))\n",
    "# simple inspect first 6\n",
    "entries[:6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99180015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic shuffle + split\n",
    "random.seed(SEED)\n",
    "entries_sorted = sorted(entries, key=lambda x: x['path'])  # stable order before shuffle\n",
    "random.shuffle(entries_sorted)\n",
    "\n",
    "# Reserve last RESERVE_COUNT items\n",
    "if RESERVE_COUNT > 0:\n",
    "    reserved = entries_sorted[-RESERVE_COUNT:]\n",
    "    remaining = entries_sorted[:-RESERVE_COUNT]\n",
    "else:\n",
    "    reserved = []\n",
    "    remaining = entries_sorted\n",
    "\n",
    "n = len(remaining)\n",
    "n_train = int(n * TRAIN_FRAC)\n",
    "n_val = int(n * VAL_FRAC)\n",
    "train = remaining[:n_train]\n",
    "val = remaining[n_train:n_train+n_val]\n",
    "test_internal = remaining[n_train+n_val:]\n",
    "\n",
    "print(\"Split sizes -> train:\", len(train), \"val:\", len(val), \"test_internal:\", len(test_internal), \"reserved:\", len(reserved))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20fd894",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = Path(OUT_DIR)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def write_list(items, filename):\n",
    "    with open(OUT_DIR / filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for it in items:\n",
    "            f.write(it['path'] + \"\\n\")\n",
    "\n",
    "write_list(train, \"train.txt\")\n",
    "write_list(val, \"val.txt\")\n",
    "write_list(test_internal, \"test_internal.txt\")\n",
    "write_list(reserved, \"reserved_200.txt\")\n",
    "\n",
    "# labels.json: map unique key -> label\n",
    "labels = {}\n",
    "for it in (train + val + test_internal + reserved):\n",
    "    key = it['stem']\n",
    "    # ensure uniqueness: if duplicate stem, append source tag\n",
    "    if key in labels:\n",
    "        key = f\"{key}__{it['source']}\"\n",
    "    labels[key] = it['label']\n",
    "\n",
    "with open(OUT_DIR / \"labels.json\", \"w\") as f:\n",
    "    json.dump(labels, f, indent=2)\n",
    "\n",
    "# manifest\n",
    "manifest = {\n",
    "    \"total_videos\": len(entries),\n",
    "    \"splits\": {\n",
    "        \"train\": len(train),\n",
    "        \"val\": len(val),\n",
    "        \"test_internal\": len(test_internal),\n",
    "        \"reserved\": len(reserved)\n",
    "    }\n",
    "}\n",
    "with open(OUT_DIR / \"data_manifest.yaml\", \"w\") as f:\n",
    "    yaml.dump(manifest, f)\n",
    "\n",
    "print(\"Wrote files to\", OUT_DIR)\n",
    "print(\"Sample labels (first 8):\")\n",
    "list(labels.items())[:8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ffade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def counts(list_items):\n",
    "    return Counter([it['label'] for it in list_items])\n",
    "\n",
    "print(\"Train label counts:\", counts(train))\n",
    "print(\"Val label counts:\", counts(val))\n",
    "print(\"Test_internal label counts:\", counts(test_internal))\n",
    "print(\"Reserved label counts:\", counts(reserved))\n",
    "print(\"Total videos:\", len(entries))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
