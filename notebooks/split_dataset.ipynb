{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c93cbe48",
   "metadata": {},
   "source": [
    "# 01 - Simple dataset split\n",
    "This notebook creates deterministic train/val/test splits and a labels.json mapping.\n",
    "It expects your preprocessed face-only folders to be present under the project root:\n",
    "- preprocessed/DFDC_REAL_Face_only_data/\n",
    "- preprocessed/DFDC_FAKE_Face_only_data/\n",
    "- preprocessed/FF_Face_only_data/  (optional metadata.csv)\n",
    "- preprocessed/Celeb_real_face_only/  (optional: not used for training)\n",
    "- preprocessed/Celeb_fake_face_only/  (optional: not used for training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ffd81d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: c:\\Users\\lkmah\\OneDrive\\Desktop\\Lokesh\\VS Code\\DeepFake_Detection_SIC\n",
      "PREPROC: c:\\Users\\lkmah\\OneDrive\\Desktop\\Lokesh\\VS Code\\DeepFake_Detection_SIC\\Dataset\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import json\n",
    "import yaml\n",
    "import pandas as pd   # used only if ffpp metadata present\n",
    "\n",
    "# --------- USER EDIT ---------\n",
    "ROOT = Path.cwd().parent\n",
    "print(\"ROOT:\", ROOT)\n",
    "PREPROC = ROOT / \"Dataset\"\n",
    "print(\"PREPROC:\", PREPROC) # where your preprocessed face-folders live\n",
    "OUT_DIR = ROOT / \"data\"         # outputs written here\n",
    "RESERVE_COUNT = 200             # internal reserved set\n",
    "SEED = 42                       # deterministic seed\n",
    "TRAIN_FRAC = 0.80\n",
    "VAL_FRAC = 0.15\n",
    "# -----------------------------\n",
    "\n",
    "# dataset folders (update names only if your folders are different)\n",
    "DFDC_REAL = PREPROC / \"DFDC_REAL_Face_only_data\"\n",
    "DFDC_FAKE = PREPROC / \"DFDC_FAKE_Face_only_data\"\n",
    "FFPP_DIR = PREPROC / \"FF_Face_only_data\"          # optional\n",
    "FFPP_META = FFPP_DIR / \"metadata.csv\"             # optional file\n",
    "#CELEB_REAL = PREPROC / \"Celeb_real_face_only\"     # optional (held-out)\n",
    "#CELEB_FAKE = PREPROC / \"Celeb_fake_face_only\"     # optional (held-out)\n",
    "\n",
    "OUT_DIR.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c37d2d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected entries: 5283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'path': 'c:\\\\Users\\\\lkmah\\\\OneDrive\\\\Desktop\\\\Lokesh\\\\VS Code\\\\DeepFake_Detection_SIC\\\\Dataset\\\\DFDC_REAL_Face_only_data\\\\aabqyygbaa.mp4',\n",
       "  'stem': 'aabqyygbaa',\n",
       "  'label': 0,\n",
       "  'source': 'dfdc_real'},\n",
       " {'path': 'c:\\\\Users\\\\lkmah\\\\OneDrive\\\\Desktop\\\\Lokesh\\\\VS Code\\\\DeepFake_Detection_SIC\\\\Dataset\\\\DFDC_REAL_Face_only_data\\\\aajsqyyjni.mp4',\n",
       "  'stem': 'aajsqyyjni',\n",
       "  'label': 0,\n",
       "  'source': 'dfdc_real'},\n",
       " {'path': 'c:\\\\Users\\\\lkmah\\\\OneDrive\\\\Desktop\\\\Lokesh\\\\VS Code\\\\DeepFake_Detection_SIC\\\\Dataset\\\\DFDC_REAL_Face_only_data\\\\aayfryxljh.mp4',\n",
       "  'stem': 'aayfryxljh',\n",
       "  'label': 0,\n",
       "  'source': 'dfdc_real'},\n",
       " {'path': 'c:\\\\Users\\\\lkmah\\\\OneDrive\\\\Desktop\\\\Lokesh\\\\VS Code\\\\DeepFake_Detection_SIC\\\\Dataset\\\\DFDC_REAL_Face_only_data\\\\abbgqbrdiz.mp4',\n",
       "  'stem': 'abbgqbrdiz',\n",
       "  'label': 0,\n",
       "  'source': 'dfdc_real'},\n",
       " {'path': 'c:\\\\Users\\\\lkmah\\\\OneDrive\\\\Desktop\\\\Lokesh\\\\VS Code\\\\DeepFake_Detection_SIC\\\\Dataset\\\\DFDC_REAL_Face_only_data\\\\abmjszfycr.mp4',\n",
       "  'stem': 'abmjszfycr',\n",
       "  'label': 0,\n",
       "  'source': 'dfdc_real'},\n",
       " {'path': 'c:\\\\Users\\\\lkmah\\\\OneDrive\\\\Desktop\\\\Lokesh\\\\VS Code\\\\DeepFake_Detection_SIC\\\\Dataset\\\\DFDC_REAL_Face_only_data\\\\abzdxpmmdb.mp4',\n",
       "  'stem': 'abzdxpmmdb',\n",
       "  'label': 0,\n",
       "  'source': 'dfdc_real'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_videos(folder: Path):\n",
    "    \"\"\"Return list of Path objects. If folder contains subfolders per video, prefer those (use subfolder name).\"\"\"\n",
    "    if not folder.exists():\n",
    "        return []\n",
    "    # if subfolders exist, treat each subfolder as one 'video' (preprocessed face folder)\n",
    "    subdirs = [p for p in folder.iterdir() if p.is_dir()]\n",
    "    if len(subdirs) > 0:\n",
    "        # use subdir path as representative; label mapping will use subdir.name as key\n",
    "        return [p for p in subdirs]\n",
    "    # else fallback: list files directly\n",
    "    files = [p for p in folder.glob(\"*\") if p.is_file()]\n",
    "    return files\n",
    "\n",
    "entries = []  # each entry: dict { \"path\": str, \"stem\": str, \"label\": 0/1, \"source\": name }\n",
    "\n",
    "# DFDC real\n",
    "for p in list_videos(DFDC_REAL):\n",
    "    entries.append({\"path\": str(p), \"stem\": p.stem, \"label\": 0, \"source\": \"dfdc_real\"})\n",
    "\n",
    "# DFDC fake\n",
    "for p in list_videos(DFDC_FAKE):\n",
    "    entries.append({\"path\": str(p), \"stem\": p.stem, \"label\": 1, \"source\": \"dfdc_fake\"})\n",
    "\n",
    "# FF++ (try metadata first if present, else infer from parent or filename)\n",
    "ff_meta = {}\n",
    "if FFPP_META.exists():\n",
    "    try:\n",
    "        df = pd.read_csv(FFPP_META)\n",
    "        # try commonly named columns\n",
    "        # prefer columns named 'video' and 'label' (if they exist)\n",
    "        if 'video' in df.columns and 'label' in df.columns:\n",
    "            for _, row in df.iterrows():\n",
    "                ff_meta[str(Path(row['video']).stem)] = int(row['label'])\n",
    "        else:\n",
    "            # build a flexible mapping: find first two meaningful columns\n",
    "            # (simple heuristic)\n",
    "            cols = list(df.columns)\n",
    "            keycol = cols[0]\n",
    "            labcol = None\n",
    "            for c in cols[1:]:\n",
    "                if any(x in c.lower() for x in ['label','fake','class','manipulated']):\n",
    "                    labcol = c\n",
    "                    break\n",
    "            if labcol is None:\n",
    "                labcol = cols[1] if len(cols) > 1 else None\n",
    "            if labcol:\n",
    "                for _, row in df.iterrows():\n",
    "                    ff_meta[str(Path(row[keycol]).stem)] = 1 if str(row[labcol]).strip().lower() in ('1','true','fake','t','y','yes') else 0\n",
    "    except Exception as e:\n",
    "        print(\"Warning: unable to parse FF++ metadata.csv:\", e)\n",
    "\n",
    "# Now list FF++ videos/folders and map labels\n",
    "for p in list_videos(FFPP_DIR):\n",
    "    stem = p.stem\n",
    "    if stem in ff_meta:\n",
    "        label = ff_meta[stem]\n",
    "    else:\n",
    "        # fallback: try to infer from foldername or parent folder\n",
    "        parent = str(p.parent).lower()\n",
    "        if 'fake' in parent:\n",
    "            label = 1\n",
    "        elif 'real' in parent:\n",
    "            label = 0\n",
    "        else:\n",
    "            # if cannot determine label, skip (safer)\n",
    "            print(f\"Skipping FF++ entry (no label): {p}\")\n",
    "            continue\n",
    "    entries.append({\"path\": str(p), \"stem\": stem, \"label\": int(label), \"source\": \"ffpp\"})\n",
    "\n",
    "'''# Celeb-DF (optional) - we include in entries but you'll not use them for training\n",
    "for p in list_videos(CELEB_REAL):\n",
    "    entries.append({\"path\": str(p), \"stem\": p.stem, \"label\": 0, \"source\": \"celeb_real\"})\n",
    "for p in list_videos(CELEB_FAKE):\n",
    "    entries.append({\"path\": str(p), \"stem\": p.stem, \"label\": 1, \"source\": \"celeb_fake\"})\n",
    "'''\n",
    "print(\"Collected entries:\", len(entries))\n",
    "# simple inspect first 6\n",
    "entries[:6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99180015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes -> train: 4066 val: 762 test_internal: 255 reserved: 200\n"
     ]
    }
   ],
   "source": [
    "# Deterministic shuffle + split\n",
    "random.seed(SEED)\n",
    "entries_sorted = sorted(entries, key=lambda x: x['path'])  # stable order before shuffle\n",
    "random.shuffle(entries_sorted)\n",
    "\n",
    "# Reserve last RESERVE_COUNT items\n",
    "if RESERVE_COUNT > 0:\n",
    "    reserved = entries_sorted[-RESERVE_COUNT:]\n",
    "    remaining = entries_sorted[:-RESERVE_COUNT]\n",
    "else:\n",
    "    reserved = []\n",
    "    remaining = entries_sorted\n",
    "\n",
    "n = len(remaining)\n",
    "n_train = int(n * TRAIN_FRAC)\n",
    "n_val = int(n * VAL_FRAC)\n",
    "train = remaining[:n_train]\n",
    "val = remaining[n_train:n_train+n_val]\n",
    "test_internal = remaining[n_train+n_val:]\n",
    "\n",
    "print(\"Split sizes -> train:\", len(train), \"val:\", len(val), \"test_internal:\", len(test_internal), \"reserved:\", len(reserved))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a20fd894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote files to c:\\Users\\lkmah\\OneDrive\\Desktop\\Lokesh\\VS Code\\DeepFake_Detection_SIC\\data\n",
      "Sample labels (first 8):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('aauhqwwncp', 1),\n",
       " ('670_661', 1),\n",
       " ('uyfdoedjoj', 0),\n",
       " ('dbrpqjttey', 1),\n",
       " ('215_208', 1),\n",
       " ('846_845', 1),\n",
       " ('450_533', 1),\n",
       " ('901', 0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUT_DIR = Path(OUT_DIR)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def write_list(items, filename):\n",
    "    with open(OUT_DIR / filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for it in items:\n",
    "            f.write(it['path'] + \"\\n\")\n",
    "\n",
    "write_list(train, \"train.txt\")\n",
    "write_list(val, \"val.txt\")\n",
    "write_list(test_internal, \"test_internal.txt\")\n",
    "write_list(reserved, \"reserved_200.txt\")\n",
    "\n",
    "# labels.json: map unique key -> label\n",
    "labels = {}\n",
    "for it in (train + val + test_internal + reserved):\n",
    "    key = it['stem']\n",
    "    # ensure uniqueness: if duplicate stem, append source tag\n",
    "    if key in labels:\n",
    "        key = f\"{key}__{it['source']}\"\n",
    "    labels[key] = it['label']\n",
    "\n",
    "with open(OUT_DIR / \"labels.json\", \"w\") as f:\n",
    "    json.dump(labels, f, indent=2)\n",
    "\n",
    "# manifest\n",
    "manifest = {\n",
    "    \"total_videos\": len(entries),\n",
    "    \"splits\": {\n",
    "        \"train\": len(train),\n",
    "        \"val\": len(val),\n",
    "        \"test_internal\": len(test_internal),\n",
    "        \"reserved\": len(reserved)\n",
    "    }\n",
    "}\n",
    "with open(OUT_DIR / \"data_manifest.yaml\", \"w\") as f:\n",
    "    yaml.dump(manifest, f)\n",
    "\n",
    "print(\"Wrote files to\", OUT_DIR)\n",
    "print(\"Sample labels (first 8):\")\n",
    "list(labels.items())[:8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5ffade9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label counts: Counter({0: 2085, 1: 1981})\n",
      "Val label counts: Counter({0: 404, 1: 358})\n",
      "Test_internal label counts: Counter({0: 131, 1: 124})\n",
      "Reserved label counts: Counter({1: 101, 0: 99})\n",
      "Total videos: 5283\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def counts(list_items):\n",
    "    return Counter([it['label'] for it in list_items])\n",
    "\n",
    "print(\"Train label counts:\", counts(train))\n",
    "print(\"Val label counts:\", counts(val))\n",
    "print(\"Test_internal label counts:\", counts(test_internal))\n",
    "print(\"Reserved label counts:\", counts(reserved))\n",
    "print(\"Total videos:\", len(entries))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c753dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total raw entries collected (len(entries)): 5283\n",
      "\n",
      "Type counts (file/dir/missing): {'file': 5283}\n",
      "\n",
      "Per-source breakdown (file/dir/missing):\n",
      "   dfdc_real {'file': 1727}\n",
      "   dfdc_fake {'file': 1566}\n",
      "   ffpp {'file': 1990}\n",
      "\n",
      "Number of duplicate stems (same stem appears multiple times across entries): 0\n",
      "\n",
      "First 12 detailed entries (inspect):\n",
      "[{'exists': True,\n",
      "  'is_dir': False,\n",
      "  'is_file': True,\n",
      "  'label': 0,\n",
      "  'path': 'c:\\\\Users\\\\lkmah\\\\OneDrive\\\\Desktop\\\\Lokesh\\\\VS '\n",
      "          'Code\\\\DeepFake_Detection_SIC\\\\Dataset\\\\DFDC_REAL_Face_only_data\\\\aabqyygbaa.mp4',\n",
      "  'source': 'dfdc_real',\n",
      "  'stem': 'aabqyygbaa'},\n",
      " {'exists': True,\n",
      "  'is_dir': False,\n",
      "  'is_file': True,\n",
      "  'label': 0,\n",
      "  'path': 'c:\\\\Users\\\\lkmah\\\\OneDrive\\\\Desktop\\\\Lokesh\\\\VS '\n",
      "          'Code\\\\DeepFake_Detection_SIC\\\\Dataset\\\\DFDC_REAL_Face_only_data\\\\aajsqyyjni.mp4',\n",
      "  'source': 'dfdc_real',\n",
      "  'stem': 'aajsqyyjni'},\n",
      " {'exists': True,\n",
      "  'is_dir': False,\n",
      "  'is_file': True,\n",
      "  'label': 0,\n",
      "  'path': 'c:\\\\Users\\\\lkmah\\\\OneDrive\\\\Desktop\\\\Lokesh\\\\VS '\n",
      "          'Code\\\\DeepFake_Detection_SIC\\\\Dataset\\\\DFDC_REAL_Face_only_data\\\\aayfryxljh.mp4',\n",
      "  'source': 'dfdc_real',\n",
      "  'stem': 'aayfryxljh'},\n",
      " {'exists': True,\n",
      "  'is_dir': False,\n",
      "  'is_file': True,\n",
      "  'label': 0,\n",
      "  'path': 'c:\\\\Users\\\\lkmah\\\\OneDrive\\\\Desktop\\\\Lokesh\\\\VS '\n",
      "          'Code\\\\DeepFake_Detection_SIC\\\\Dataset\\\\DFDC_REAL_Face_only_data\\\\abbgqbrdiz.mp4',\n",
      "  'source': 'dfdc_real',\n",
      "  'stem': 'abbgqbrdiz'},\n",
      " {'exists': True,\n",
      "  'is_dir': False,\n",
      "  'is_file': True,\n",
      "  'label': 0,\n",
      "  'path': 'c:\\\\Users\\\\lkmah\\\\OneDrive\\\\Desktop\\\\Lokesh\\\\VS '\n",
      "          'Code\\\\DeepFake_Detection_SIC\\\\Dataset\\\\DFDC_REAL_Face_only_data\\\\abmjszfycr.mp4',\n",
      "  'source': 'dfdc_real',\n",
      "  'stem': 'abmjszfycr'},\n",
      " {'exists': True,\n",
      "  'is_dir': False,\n",
      "  'is_file': True,\n",
      "  'label': 0,\n",
      "  'path': 'c:\\\\Users\\\\lkmah\\\\OneDrive\\\\Desktop\\\\Lokesh\\\\VS '\n",
      "          'Code\\\\DeepFake_Detection_SIC\\\\Dataset\\\\DFDC_REAL_Face_only_data\\\\abzdxpmmdb.mp4',\n",
      "  'source': 'dfdc_real',\n",
      "  'stem': 'abzdxpmmdb'},\n",
      " {'exists': True,\n",
      "  'is_dir': False,\n",
      "  'is_file': True,\n",
      "  'label': 0,\n",
      "  'path': 'c:\\\\Users\\\\lkmah\\\\OneDrive\\\\Desktop\\\\Lokesh\\\\VS '\n",
      "          'Code\\\\DeepFake_Detection_SIC\\\\Dataset\\\\DFDC_REAL_Face_only_data\\\\acgzblwsgw.mp4',\n",
      "  'source': 'dfdc_real',\n",
      "  'stem': 'acgzblwsgw'},\n",
      " {'exists': True,\n",
      "  'is_dir': False,\n",
      "  'is_file': True,\n",
      "  'label': 0,\n",
      "  'path': 'c:\\\\Users\\\\lkmah\\\\OneDrive\\\\Desktop\\\\Lokesh\\\\VS '\n",
      "          'Code\\\\DeepFake_Detection_SIC\\\\Dataset\\\\DFDC_REAL_Face_only_data\\\\addknoverp.mp4',\n",
      "  'source': 'dfdc_real',\n",
      "  'stem': 'addknoverp'},\n",
      " {'exists': True,\n",
      "  'is_dir': False,\n",
      "  'is_file': True,\n",
      "  'label': 0,\n",
      "  'path': 'c:\\\\Users\\\\lkmah\\\\OneDrive\\\\Desktop\\\\Lokesh\\\\VS '\n",
      "          'Code\\\\DeepFake_Detection_SIC\\\\Dataset\\\\DFDC_REAL_Face_only_data\\\\adgveaibmt.mp4',\n",
      "  'source': 'dfdc_real',\n",
      "  'stem': 'adgveaibmt'},\n",
      " {'exists': True,\n",
      "  'is_dir': False,\n",
      "  'is_file': True,\n",
      "  'label': 0,\n",
      "  'path': 'c:\\\\Users\\\\lkmah\\\\OneDrive\\\\Desktop\\\\Lokesh\\\\VS '\n",
      "          'Code\\\\DeepFake_Detection_SIC\\\\Dataset\\\\DFDC_REAL_Face_only_data\\\\adkymkuove.mp4',\n",
      "  'source': 'dfdc_real',\n",
      "  'stem': 'adkymkuove'},\n",
      " {'exists': True,\n",
      "  'is_dir': False,\n",
      "  'is_file': True,\n",
      "  'label': 0,\n",
      "  'path': 'c:\\\\Users\\\\lkmah\\\\OneDrive\\\\Desktop\\\\Lokesh\\\\VS '\n",
      "          'Code\\\\DeepFake_Detection_SIC\\\\Dataset\\\\DFDC_REAL_Face_only_data\\\\adohdulfwb.mp4',\n",
      "  'source': 'dfdc_real',\n",
      "  'stem': 'adohdulfwb'},\n",
      " {'exists': True,\n",
      "  'is_dir': False,\n",
      "  'is_file': True,\n",
      "  'label': 0,\n",
      "  'path': 'c:\\\\Users\\\\lkmah\\\\OneDrive\\\\Desktop\\\\Lokesh\\\\VS '\n",
      "          'Code\\\\DeepFake_Detection_SIC\\\\Dataset\\\\DFDC_REAL_Face_only_data\\\\advzryyfkn.mp4',\n",
      "  'source': 'dfdc_real',\n",
      "  'stem': 'advzryyfkn'}]\n",
      "\n",
      "Wrote debug_entries.json (first 100 entries) for further inspection.\n"
     ]
    }
   ],
   "source": [
    "'''# DEBUG: show exactly what entries were collected and whether they are files or dirs\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "\n",
    "# re-use 'entries' from your notebook if available, otherwise rebuild minimal list\n",
    "if \"entries\" not in globals():\n",
    "    print(\"`entries` not found in notebook namespace. Re-run the collection cell first.\")\n",
    "else:\n",
    "    print(\"Total raw entries collected (len(entries)):\", len(entries))\n",
    "\n",
    "    # count by whether path is file or dir (on disk)\n",
    "    type_counts = Counter()\n",
    "    per_source_counts = defaultdict(lambda: Counter())\n",
    "    stem_counts = Counter()\n",
    "    seen_paths = []\n",
    "    detailed = []\n",
    "\n",
    "    for it in entries:\n",
    "        p = Path(it['path'])\n",
    "        is_file = p.is_file()\n",
    "        is_dir  = p.is_dir()\n",
    "        t = \"file\" if is_file else (\"dir\" if is_dir else \"missing\")\n",
    "        type_counts[t] += 1\n",
    "        per_source_counts[it['source']][t] += 1\n",
    "        stem_counts[it['stem']] += 1\n",
    "        seen_paths.append(str(p))\n",
    "        detailed.append({\"path\": str(p), \"exists\": p.exists(), \"is_file\": is_file, \"is_dir\": is_dir,\n",
    "                         \"stem\": it['stem'], \"label\": it['label'], \"source\": it['source']})\n",
    "\n",
    "    print(\"\\nType counts (file/dir/missing):\", dict(type_counts))\n",
    "    print(\"\\nPer-source breakdown (file/dir/missing):\")\n",
    "    for src, cnt in per_source_counts.items():\n",
    "        print(\"  \", src, dict(cnt))\n",
    "\n",
    "    # find stems that appear more than once (possible duplicates)\n",
    "    dup_stems = [s for s,c in stem_counts.items() if c > 1]\n",
    "    print(\"\\nNumber of duplicate stems (same stem appears multiple times across entries):\", len(dup_stems))\n",
    "    if len(dup_stems) > 0:\n",
    "        print(\"Sample duplicate stems and their entries (first 10):\")\n",
    "        sample = dup_stems[:10]\n",
    "        for s in sample:\n",
    "            print(\"  STEM:\", s)\n",
    "            for it in [d for d in detailed if d['stem']==s]:\n",
    "                print(\"    \", it)\n",
    "\n",
    "    # show first 12 raw detailed entries to inspect\n",
    "    print(\"\\nFirst 12 detailed entries (inspect):\")\n",
    "    from pprint import pprint\n",
    "    pprint(detailed[:12])\n",
    "\n",
    "    # write debug dump to file for inspection\n",
    "    with open(\"debug_entries.json\",\"w\") as f:\n",
    "        json.dump(detailed, f, indent=2)\n",
    "    print(\"\\nWrote debug_entries.json (first 100 entries) for further inspection.\")'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
