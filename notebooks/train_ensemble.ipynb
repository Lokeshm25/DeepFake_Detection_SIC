{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78a7740d",
   "metadata": {},
   "source": [
    "# train_ensemble_final.py\n",
    "\n",
    "Robust ensemble trainer (Spatial + Temporal) — final, stable version.\n",
    "\n",
    "Behavior:\n",
    "- If predictions/ (spatial + temporal) exist, uses them (fast).\n",
    "- Otherwise computes features from embeddings using spatial head + temporal model.\n",
    "- Logs bad files to OUT_DIR/bad_embeddings.txt and OUT_DIR/bad_preds.txt\n",
    "- Trains a calibrated logistic regression (if calibration possible), otherwise a plain LR.\n",
    "- Saves trained ensemble and a small results text file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc137c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, time, warnings, sys\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# sklearn / torch / joblib imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b62836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- CONFIG ----------------\n",
    "ROOT = Path.cwd().parent\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "EMB_ROOT = ROOT / \"embeddings\"                 # embeddings/<split>/<stem>.npy\n",
    "PRED_SPATIAL_DIR = ROOT / \"predictions\" / \"spatial\"    # optional: per-video saved scalar preds\n",
    "PRED_TEMPORAL_DIR = ROOT / \"predictions\" / \"temporal\"\n",
    "\n",
    "CHECKPOINT_DIR = ROOT / \"checkpoints\" / \"ensemble\"\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_CACHE = ROOT / \"ensemble_features\"        # caches X/y as npz\n",
    "OUT_CACHE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SPATIAL_CKPT = ROOT / \"checkpoints\" / \"spatial\" / \"spatial_best_valAUC.pth\"\n",
    "TEMPORAL_CKPT = ROOT / \"checkpoints\" / \"temporal\" / \"temporal_best.pth\"  # or temporal_best.pth\n",
    "\n",
    "LABELS_JSON = DATA_DIR / \"labels.json\"\n",
    "\n",
    "BAD_EMB_LOG = CHECKPOINT_DIR / \"bad_embeddings.txt\"\n",
    "BAD_PRED_LOG = CHECKPOINT_DIR / \"bad_preds.txt\"\n",
    "RESULTS_TXT = CHECKPOINT_DIR / \"ensemble_results.txt\"\n",
    "ENSEMBLE_OUT = CHECKPOINT_DIR / \"ensemble_best.pkl\"\n",
    "\n",
    "SPLITS = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "# Ensemble / training hyperparams\n",
    "RNG_SEED = 42\n",
    "CALIBRATION_CV = 5        # fallback to smaller value if dataset small\n",
    "LR_MAX_ITER = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d203ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ determinism ------------\n",
    "def set_seed(seed=RNG_SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # deterministic cudnn (may slow)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(RNG_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70527a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ helpers ------------\n",
    "def safe_load_checkpoint(path):\n",
    "    if not Path(path).exists():\n",
    "        return None\n",
    "    ck = torch.load(path, map_location=\"cpu\")\n",
    "    return ck\n",
    "\n",
    "def strip_module_prefix(state_dict):\n",
    "    new = {}\n",
    "    for k, v in state_dict.items():\n",
    "        nk = k.replace(\"module.\", \"\") if k.startswith(\"module.\") else k\n",
    "        new[nk] = v\n",
    "    return new\n",
    "\n",
    "def is_bad_array(arr):\n",
    "    \"\"\"Check numpy arr for problems; returns (bad_bool, reason_str)\"\"\"\n",
    "    if not isinstance(arr, np.ndarray):\n",
    "        return True, f\"[NOT_NDARRAY] type={type(arr)}\"\n",
    "    if arr.ndim != 2:\n",
    "        return True, f\"[DIM] ndim={arr.ndim} shape={getattr(arr,'shape',None)}\"\n",
    "    if arr.shape[0] == 0:\n",
    "        return True, f\"[EMPTY] shape={arr.shape}\"\n",
    "    if np.isnan(arr).any():\n",
    "        return True, f\"[NaN] min={np.nanmin(arr)} max={np.nanmax(arr)} mean={np.nanmean(arr)}\"\n",
    "    if np.isinf(arr).any():\n",
    "        return True, \"[INF]\"\n",
    "    # extreme values guard\n",
    "    mx = np.max(np.abs(arr))\n",
    "    if np.isnan(mx) or mx > 1e6:\n",
    "        return True, f\"[EXTREME] abs_max={mx}\"\n",
    "    return False, \"\"\n",
    "\n",
    "def safe_auc(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    if np.isnan(y_pred).any():\n",
    "        warnings.warn(\"NaNs in predictions — skipping AUC\")\n",
    "        return float(\"nan\")\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        warnings.warn(f\"Single-class y_true={np.unique(y_true)} — AUC undefined\")\n",
    "        return float(\"nan\")\n",
    "    return roc_auc_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22629740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ Model classes used if we compute features ------------\n",
    "class SpatialHead(nn.Module):\n",
    "    def __init__(self, backbone_name=\"efficientnet_b3\"):\n",
    "        super().__init__()\n",
    "        backbone = timm.create_model(backbone_name, pretrained=False, num_classes=0)\n",
    "        self.backbone = backbone\n",
    "        self.feat_dim = backbone.num_features\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.feat_dim, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)\n",
    "        return self.head(feats).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc48f412",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalModel(nn.Module):\n",
    "    def __init__(self, feat_dim, hidden=512, layers=2, dropout=0.3, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(feat_dim, hidden, layers, batch_first=True,\n",
    "                            bidirectional=bidirectional, dropout=dropout if layers>1 else 0)\n",
    "        self.out_dim = hidden * (2 if bidirectional else 1)\n",
    "        self.att = nn.Linear(self.out_dim, 1)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.out_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    def forward(self, x, lengths):\n",
    "        # x: [B, T, feat], lengths: [B]\n",
    "        # pack for safe RNN handling\n",
    "        if lengths.numel() == 0:\n",
    "            raise ValueError(\"Empty lengths in TemporalModel\")\n",
    "        lengths_sorted, perm = lengths.sort(descending=True)\n",
    "        x_sorted = x[perm]\n",
    "        packed = rnn_utils.pack_padded_sequence(x_sorted, lengths_sorted.cpu(), batch_first=True, enforce_sorted=True)\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        out_unpacked, _ = rnn_utils.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        # unsort\n",
    "        _, unperm = perm.sort()\n",
    "        out = out_unpacked[unperm]\n",
    "        lengths = lengths[unperm]\n",
    "        # attention pooling with mask\n",
    "        B, T, H = out.shape\n",
    "        scores = self.att(out).squeeze(-1)   # [B, T]\n",
    "        mask = torch.arange(T, device=out.device).unsqueeze(0) >= lengths.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask, -1e9)\n",
    "        weights = torch.softmax(scores, dim=1)\n",
    "        weights = torch.nan_to_num(weights, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        pooled = (out * weights.unsqueeze(-1)).sum(dim=1)\n",
    "        logits = self.head(pooled).squeeze(1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff8647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ Feature builders ------------\n",
    "def build_features_from_embeddings(split, spatial_ckpt=SPATIAL_CKPT, temporal_ckpt=TEMPORAL_CKPT, overwrite=False):\n",
    "    \"\"\"\n",
    "    Build features (mean, max, std, top3, temporal_score) from embeddings/<split>/*.npy\n",
    "    Saves cache to OUT_CACHE/<split>.npz\n",
    "    Returns X (N x 5), y (N,)\n",
    "    \"\"\"\n",
    "    cache_path = OUT_CACHE / f\"{split}.npz\"\n",
    "    if cache_path.exists() and not overwrite:\n",
    "        print(f\"Loading cached features for {split} from {cache_path}\")\n",
    "        d = np.load(cache_path, allow_pickle=True)\n",
    "        return d[\"X\"], d[\"y\"]\n",
    "\n",
    "    # load labels map\n",
    "    with open(LABELS_JSON, \"r\") as f:\n",
    "        labels_map = json.load(f)\n",
    "\n",
    "    # instantiate spatial head and temporal model if available\n",
    "    spatial = None\n",
    "    temporal = None\n",
    "    spatial_state = safe_load_checkpoint(spatial_ckpt)\n",
    "    if spatial_state is not None:\n",
    "        # load backbone features-only weights if possible\n",
    "        state = spatial_state.get(\"model_state\", spatial_state)\n",
    "        state = strip_module_prefix(state)\n",
    "        # create a SpatialHead and try to load any matching keys\n",
    "        spatial = SpatialHead()\n",
    "        try:\n",
    "            # try load whole state (non-strict)\n",
    "            spatial.load_state_dict(state, strict=False)\n",
    "            print(\"Spatial checkpoint loaded into SpatialHead (non-strict).\")\n",
    "        except Exception:\n",
    "            # ignore: we'll still use backbone features via timm\n",
    "            print(\"Warning: couldn't fully load spatial checkpoint; proceeding with backbone defaults.\")\n",
    "        spatial.eval()\n",
    "        spatial.to(\"cpu\")  # we'll move tensors as needed later\n",
    "\n",
    "    temporal_state = safe_load_checkpoint(temporal_ckpt)\n",
    "    if temporal_state is not None:\n",
    "        tstate = temporal_state.get(\"model_state\", temporal_state)\n",
    "        tstate = strip_module_prefix(tstate)\n",
    "        # infer feat_dim from a sample embedding (need at least one file)\n",
    "        # We'll load temporal after we infer FEAT_DIM below.\n",
    "        print(\"Temporal checkpoint loaded (keys found).\")\n",
    "    else:\n",
    "        tstate = None\n",
    "\n",
    "    split_dir = EMB_ROOT / split\n",
    "    if not split_dir.exists():\n",
    "        raise RuntimeError(f\"No embeddings directory for split: {split_dir}\")\n",
    "\n",
    "    files = sorted(split_dir.glob(\"*.npy\"))\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    # clear bad log\n",
    "    if BAD_EMB_LOG.exists():\n",
    "        BAD_EMB_LOG.unlink()\n",
    "\n",
    "    for p in files:\n",
    "        try:\n",
    "            arr = np.load(p)\n",
    "        except Exception as e:\n",
    "            with open(BAD_EMB_LOG, \"a\") as f:\n",
    "                f.write(f\"[LOAD_ERROR] {p} | {e}\\n\")\n",
    "            continue\n",
    "\n",
    "        bad, reason = is_bad_array(arr)\n",
    "        if bad:\n",
    "            with open(BAD_EMB_LOG, \"a\") as f:\n",
    "                f.write(f\"{p} | {reason}\\n\")\n",
    "            continue\n",
    "\n",
    "        # spatial stats: compute logits by treating each row as feat vector into spatial head.head\n",
    "        try:\n",
    "            if spatial is not None:\n",
    "                # we assume spatial.backbone produces same feature dim as embeddings columns\n",
    "                emb_t = torch.from_numpy(arr.astype(np.float32))\n",
    "                with torch.no_grad():\n",
    "                    logits = spatial.head(emb_t) if hasattr(spatial, \"head\") else torch.zeros(len(emb_t))\n",
    "                    # defensive: ensure numeric\n",
    "                    if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "                        logits = torch.nan_to_num(logits, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                    probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            else:\n",
    "                # If no spatial checkpoint, we can't run head; derive proxy by using row-norms (safe fallback)\n",
    "                norms = np.linalg.norm(arr, axis=1)\n",
    "                probs = (norms - norms.min()) / (norms.max() - norms.min() + 1e-12)\n",
    "        except Exception as e:\n",
    "            with open(BAD_EMB_LOG, \"a\") as f:\n",
    "                f.write(f\"[SPATIAL_EVAL_ERROR] {p} | {e}\\n\")\n",
    "            continue\n",
    "\n",
    "        s_mean = float(probs.mean())\n",
    "        s_max  = float(probs.max())\n",
    "        s_std  = float(probs.std())\n",
    "        s_top3 = float(np.sort(probs)[-3:].mean()) if len(probs) >= 3 else s_mean\n",
    "\n",
    "        # temporal score\n",
    "        try:\n",
    "            # simple single-sample inference: build a tiny temporal model if checkpoint exists,\n",
    "            # otherwise approximate by e.g., mean of probs\n",
    "            if tstate is not None:\n",
    "                # lazy create temporal model on first call\n",
    "                if 'temporal_model_cached' not in globals():\n",
    "                    FEAT_DIM = arr.shape[1]\n",
    "                    globals()['temporal_model_cached'] = TemporalModel(FEAT_DIM)\n",
    "                    globals()['temporal_model_cached'].load_state_dict(tstate, strict=False)\n",
    "                    globals()['temporal_model_cached'].eval()\n",
    "                tm = globals()['temporal_model_cached']\n",
    "                x = torch.from_numpy(arr.astype(np.float32)).unsqueeze(0)  # [1,T,feat]\n",
    "                lengths = torch.tensor([arr.shape[0]], dtype=torch.long)\n",
    "                with torch.no_grad():\n",
    "                    logits = tm(x, lengths)\n",
    "                    logits = torch.nan_to_num(logits, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                    t_prob = float(torch.sigmoid(logits).item())\n",
    "            else:\n",
    "                # fallback: use mean of spatial probs as temporal proxy\n",
    "                t_prob = float(s_mean)\n",
    "        except Exception as e:\n",
    "            with open(BAD_EMB_LOG, \"a\") as f:\n",
    "                f.write(f\"[TEMPORAL_EVAL_ERROR] {p} | {e}\\n\")\n",
    "            continue\n",
    "\n",
    "        feat = [s_mean, s_max, s_std, s_top3, t_prob]\n",
    "        X_list.append(feat)\n",
    "        # label lookup\n",
    "        stem = p.stem\n",
    "        try:\n",
    "            with open(LABELS_JSON, \"r\") as f:\n",
    "                labels_map = json.load(f)\n",
    "            lab = labels_map.get(stem, None)\n",
    "            if lab is None:\n",
    "                # fallback to substring match\n",
    "                for k,v in labels_map.items():\n",
    "                    if stem in k:\n",
    "                        lab = int(v); break\n",
    "            if lab is None:\n",
    "                raise KeyError(\"Label missing\")\n",
    "            y_list.append(int(lab))\n",
    "        except Exception as e:\n",
    "            with open(BAD_EMB_LOG, \"a\") as f:\n",
    "                f.write(f\"[LABEL_ERROR] {p} | {e}\\n\")\n",
    "            # skip if label missing\n",
    "            continue\n",
    "\n",
    "    if len(X_list) == 0:\n",
    "        raise RuntimeError(f\"No valid features built for split {split}. Check {BAD_EMB_LOG}\")\n",
    "\n",
    "    X = np.array(X_list, dtype=np.float32)\n",
    "    y = np.array(y_list, dtype=np.int64)\n",
    "    np.savez(cache_path, X=X, y=y)\n",
    "    print(f\"Saved feature cache: {cache_path} (n={len(y)})\")\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6257775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features_from_predictions(split, overwrite=False):\n",
    "    \"\"\"\n",
    "    Load per-video scalar predictions from PRED_SPATIAL_DIR/<split>/<stem>.npy\n",
    "    and PRED_TEMPORAL_DIR/<split>/<stem>.npy\n",
    "    Save cache to OUT_CACHE/<split>.npz\n",
    "    \"\"\"\n",
    "    cache_path = OUT_CACHE / f\"{split}.npz\"\n",
    "    if cache_path.exists() and not overwrite:\n",
    "        print(f\"Loaded cached features for {split} from {cache_path}\")\n",
    "        d = np.load(cache_path, allow_pickle=True)\n",
    "        return d[\"X\"], d[\"y\"]\n",
    "\n",
    "    spatial_dir = PRED_SPATIAL_DIR / split\n",
    "    temporal_dir = PRED_TEMPORAL_DIR / split\n",
    "    if not spatial_dir.exists() or not temporal_dir.exists():\n",
    "        raise RuntimeError(\"Prediction directories missing; cannot build from predictions\")\n",
    "\n",
    "    if BAD_PRED_LOG.exists():\n",
    "        BAD_PRED_LOG.unlink()\n",
    "\n",
    "    spatial_files = {p.stem:p for p in spatial_dir.glob(\"*.npy\")}\n",
    "    temporal_files = {p.stem:p for p in temporal_dir.glob(\"*.npy\")}\n",
    "    stems = sorted(set(spatial_files.keys()) & set(temporal_files.keys()))\n",
    "    if not stems:\n",
    "        raise RuntimeError(\"No common prediction stems found between spatial and temporal predictions\")\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "    with open(LABELS_JSON, \"r\") as f:\n",
    "        labels_map = json.load(f)\n",
    "\n",
    "    for stem in stems:\n",
    "        try:\n",
    "            s_v = np.load(spatial_files[stem], allow_pickle=True)\n",
    "            # convert to scalar safely\n",
    "            if np.isscalar(s_v):\n",
    "                s = float(s_v)\n",
    "            elif isinstance(s_v, np.ndarray) and s_v.size == 1:\n",
    "                s = float(s_v.flatten()[0])\n",
    "            else:\n",
    "                raise ValueError(f\"Bad spatial pred shape: {getattr(s_v,'shape',None)}\")\n",
    "            t_v = np.load(temporal_files[stem], allow_pickle=True)\n",
    "            if np.isscalar(t_v):\n",
    "                t = float(t_v)\n",
    "            elif isinstance(t_v, np.ndarray) and t_v.size == 1:\n",
    "                t = float(t_v.flatten()[0])\n",
    "            else:\n",
    "                raise ValueError(f\"Bad temporal pred shape: {getattr(t_v,'shape',None)}\")\n",
    "            if not np.isfinite(s) or not np.isfinite(t):\n",
    "                raise ValueError(f\"Non-finite pred s={s}, t={t}\")\n",
    "        except Exception as e:\n",
    "            with open(BAD_PRED_LOG, \"a\") as f:\n",
    "                f.write(f\"{stem} | {e}\\n\")\n",
    "            continue\n",
    "\n",
    "        feat = [s, t]          # only two features; we'll pad to 5 with zeros for consistency\n",
    "        # map label\n",
    "        lab = labels_map.get(stem, None)\n",
    "        if lab is None:\n",
    "            for k,v in labels_map.items():\n",
    "                if stem in k:\n",
    "                    lab = int(v); break\n",
    "        if lab is None:\n",
    "            with open(BAD_PRED_LOG, \"a\") as f:\n",
    "                f.write(f\"{stem} | LABEL_MISSING\\n\")\n",
    "            continue\n",
    "\n",
    "        # expand to 5 dims: [mean,max,std,top3,temporal] where spatial stats unknown => use s repeated\n",
    "        X_list.append([s, s, 0.0, s, t])\n",
    "        y_list.append(int(lab))\n",
    "\n",
    "    if len(X_list) == 0:\n",
    "        raise RuntimeError(f\"No valid prediction-based features for split {split}. Check {BAD_PRED_LOG}\")\n",
    "\n",
    "    X = np.array(X_list, dtype=np.float32)\n",
    "    y = np.array(y_list, dtype=np.int64)\n",
    "    np.savez(cache_path, X=X, y=y)\n",
    "    print(f\"Saved prediction-based cache: {cache_path} (n={len(y)})\")\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd15209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ Determine mode (predictions exist?) ------------\n",
    "def have_prediction_dirs():\n",
    "    return (PRED_SPATIAL_DIR.exists() and PRED_TEMPORAL_DIR.exists())\n",
    "\n",
    "mode_from_preds = have_prediction_dirs()\n",
    "print(\"Mode:\", \"predictions\" if mode_from_preds else \"embeddings (will compute features)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4e96fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ Build / load features for all splits ------------\n",
    "def build_all_features(overwrite=False):\n",
    "    Xs, ys = {}, {}\n",
    "    if mode_from_preds:\n",
    "        builder = build_features_from_predictions\n",
    "    else:\n",
    "        builder = build_features_from_embeddings\n",
    "\n",
    "    for split in SPLITS:\n",
    "        try:\n",
    "            Xs[split], ys[split] = builder(split, overwrite=overwrite)\n",
    "        except Exception as e:\n",
    "            print(f\"Error building features for split={split}: {e}. Check logs {BAD_EMB_LOG if not mode_from_preds else BAD_PRED_LOG}\")\n",
    "            Xs[split], ys[split] = np.zeros((0,5), dtype=np.float32), np.zeros((0,), dtype=np.int64)\n",
    "    return Xs, ys\n",
    "\n",
    "X_dict, y_dict = build_all_features(overwrite=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bffada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick dataset stats\n",
    "print(\"Train size:\", X_dict[\"train\"].shape[0], \"Val size:\", X_dict[\"val\"].shape[0], \"Test size:\", X_dict[\"test\"].shape[0])\n",
    "print(\"Train label counts:\", Counter(y_dict[\"train\"].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2877829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ Train calibrated logistic regression (with fallbacks) ------------\n",
    "# Prepare training arrays\n",
    "X_train = X_dict[\"train\"]\n",
    "y_train = y_dict[\"train\"]\n",
    "X_val = X_dict[\"val\"]\n",
    "y_val = y_dict[\"val\"]\n",
    "X_test = X_dict[\"test\"]\n",
    "y_test = y_dict[\"test\"]\n",
    "\n",
    "if len(y_train) == 0:\n",
    "    raise RuntimeError(\"No training samples available. Cannot train ensemble.\")\n",
    "\n",
    "# choose cv safely\n",
    "n_samples = len(y_train)\n",
    "cv = CALIBRATION_CV\n",
    "while cv > 1:\n",
    "    # ensure each fold will have at least 1 sample per class roughly\n",
    "    if n_samples >= cv * 2:  # heuristic: want at least 2 samples per fold\n",
    "        break\n",
    "    cv -= 1\n",
    "if cv < 2:\n",
    "    cv = None  # don't calibrate with cross-val if too small\n",
    "\n",
    "base_clf = LogisticRegression(max_iter=LR_MAX_ITER, solver=\"lbfgs\")\n",
    "ensemble_clf = None\n",
    "\n",
    "if cv is None:\n",
    "    print(\"Calibration cv too small — training plain LogisticRegression (no calibration).\")\n",
    "    base_clf.fit(X_train, y_train)\n",
    "    ensemble_clf = base_clf\n",
    "else:\n",
    "    try:\n",
    "        skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RNG_SEED)\n",
    "        ensemble = CalibratedClassifierCV(base_clf, cv=skf, method=\"sigmoid\")\n",
    "        ensemble.fit(X_train, y_train)\n",
    "        ensemble_clf = ensemble\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"CalibratedClassifierCV failed: {e}. Falling back to plain LogisticRegression.\")\n",
    "        base_clf.fit(X_train, y_train)\n",
    "        ensemble_clf = base_clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a896d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ Evaluate and save ------------\n",
    "train_p = ensemble_clf.predict_proba(X_train)[:,1] if hasattr(ensemble_clf, \"predict_proba\") else ensemble_clf.decision_function(X_train)\n",
    "val_p   = ensemble_clf.predict_proba(X_val)[:,1] if X_val.shape[0]>0 and hasattr(ensemble_clf, \"predict_proba\") else (ensemble_clf.decision_function(X_val) if X_val.shape[0]>0 else np.array([]))\n",
    "test_p  = ensemble_clf.predict_proba(X_test)[:,1] if X_test.shape[0]>0 and hasattr(ensemble_clf, \"predict_proba\") else (ensemble_clf.decision_function(X_test) if X_test.shape[0]>0 else np.array([]))\n",
    "\n",
    "train_auc = safe_auc(y_train, train_p)\n",
    "val_auc   = safe_auc(y_val, val_p) if X_val.shape[0]>0 else float(\"nan\")\n",
    "test_auc  = safe_auc(y_test, test_p) if X_test.shape[0]>0 else float(\"nan\")\n",
    "\n",
    "# Save model and results\n",
    "joblib.dump(ensemble_clf, ENSEMBLE_OUT)\n",
    "with open(RESULTS_TXT, \"w\") as f:\n",
    "    f.write(f\"Mode: {'predictions' if mode_from_preds else 'embeddings'}\\n\")\n",
    "    f.write(f\"Train samples: {len(y_train)}  Train AUC: {train_auc}\\n\")\n",
    "    f.write(f\"Val samples:   {len(y_val)}  Val AUC:   {val_auc}\\n\")\n",
    "    f.write(f\"Test samples:  {len(y_test)}  Test AUC:  {test_auc}\\n\")\n",
    "    f.write(\"\\nNotes:\\n\")\n",
    "    if BAD_EMB_LOG.exists():\n",
    "        f.write(f\"Bad embeddings log: {BAD_EMB_LOG}\\n\")\n",
    "    if BAD_PRED_LOG.exists():\n",
    "        f.write(f\"Bad preds log: {BAD_PRED_LOG}\\n\")\n",
    "print(\"Saved ensemble to:\", ENSEMBLE_OUT)\n",
    "print(\"Results written to:\", RESULTS_TXT)\n",
    "if BAD_EMB_LOG.exists():\n",
    "    print(\"Bad embeddings logged at:\", BAD_EMB_LOG)\n",
    "if BAD_PRED_LOG.exists():\n",
    "    print(\"Bad predictions logged at:\", BAD_PRED_LOG)\n",
    "\n",
    "# Print summary to console\n",
    "print(\"Train AUC:\", train_auc)\n",
    "print(\"Val AUC:\", val_auc)\n",
    "print(\"Test AUC:\", test_auc)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
