{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4975595",
   "metadata": {},
   "source": [
    "# 06 - Train temporal model (LSTM + attention) on embeddings\n",
    "Trains a video-level LSTM aggregator using per-frame embeddings produced by the spatial model.\n",
    "Saves checkpoints: checkpoints/temporal/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b4bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, time\n",
    "import random\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# ------------- USER CONFIG -------------\n",
    "ROOT = Path.cwd().parent\n",
    "EMB_ROOT = ROOT / \"embeddings\"             # embeddings/<split>/<video_stem>.npy\n",
    "LABELS_JSON = ROOT / \"data\" / \"labels.json\"\n",
    "CHECKPOINT_DIR = ROOT / \"checkpoints\" / \"temporal\"\n",
    "NUM_EPOCHS = 25\n",
    "BATCH_SIZE = 16            # number of videos per batch\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NUM_WORKERS = 0            # keep 0 in notebooks; increase on robust machines\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PRINT_FREQ = 20\n",
    "LSTM_HIDDEN = 512\n",
    "LSTM_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "ATTENTION = True           # use attention pooling over LSTM outputs\n",
    "BIDIRECTIONAL = True\n",
    "# ---------------------------------------\n",
    "\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Device:\", DEVICE)\n",
    "print(\"Emb root:\", EMB_ROOT)\n",
    "print(\"Checkpoint dir:\", CHECKPOINT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2724a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "with open(LABELS_JSON, \"r\") as f:\n",
    "    labels_map = json.load(f)\n",
    "\n",
    "def get_label_from_stem(stem):\n",
    "    if stem in labels_map:\n",
    "        return int(labels_map[stem])\n",
    "    for k,v in labels_map.items():\n",
    "        if stem in k:\n",
    "            return int(v)\n",
    "    raise KeyError(f\"Label for {stem} not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d98f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoEmbeddingDataset(Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.root = EMB_ROOT / split\n",
    "        if not self.root.exists():\n",
    "            raise RuntimeError(f\"No embeddings for split: {split}\")\n",
    "        self.items = sorted([p for p in self.root.glob(\"*.npy\")])\n",
    "        # optional: filter if empty\n",
    "        self.items = [p for p in self.items if p.stat().st_size > 0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.items[idx]\n",
    "        stem = p.stem\n",
    "        arr = np.load(p)                  # shape (T, feat_dim)\n",
    "        # convert to float32 and torch tensor\n",
    "        emb = torch.from_numpy(arr.astype(np.float32))  # [T,feat]\n",
    "        label = get_label_from_stem(stem)\n",
    "        return emb, torch.tensor(label, dtype=torch.float32), stem\n",
    "\n",
    "# quick sanity\n",
    "# ds = VideoEmbeddingDataset(\"train\")\n",
    "# print(\"Train videos:\", len(ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb92afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: list of (emb [T,feat], label, stem)\n",
    "    Pads sequences to longest T in batch (simple zero padding).\n",
    "    Returns tensors: seqs [B, Tmax, feat], lengths [B], labels [B]\n",
    "    \"\"\"\n",
    "    seqs, labels, stems = zip(*batch)\n",
    "    lengths = [s.shape[0] for s in seqs]\n",
    "    maxlen = max(lengths)\n",
    "    feat_dim = seqs[0].shape[1]\n",
    "    out = torch.zeros(len(seqs), maxlen, feat_dim, dtype=torch.float32)\n",
    "    for i, s in enumerate(seqs):\n",
    "        out[i, :s.shape[0], :] = s\n",
    "    labels = torch.stack(labels)\n",
    "    return out, torch.tensor(lengths, dtype=torch.long), labels, list(stems)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08c8045",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPool(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.att = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, h, lengths):\n",
    "        # h: [B, T, H]\n",
    "        scores = self.att(h).squeeze(-1)      # [B, T]\n",
    "        # mask\n",
    "        mask = torch.arange(h.size(1), device=h.device).unsqueeze(0) >= lengths.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask, float(\"-1e9\"))\n",
    "        weights = torch.softmax(scores, dim=1)   # [B, T]\n",
    "        out = (h * weights.unsqueeze(-1)).sum(dim=1)  # [B, H]\n",
    "        return out, weights\n",
    "\n",
    "class TemporalModel(nn.Module):\n",
    "    def __init__(self, feat_dim, hidden_dim=LSTM_HIDDEN, n_layers=LSTM_LAYERS,\n",
    "                 bidirectional=BIDIRECTIONAL, dropout=DROPOUT, use_attn=ATTENTION):\n",
    "        super().__init__()\n",
    "        self.use_attn = use_attn\n",
    "        self.lstm = nn.LSTM(input_size=feat_dim, hidden_size=hidden_dim, num_layers=n_layers,\n",
    "                            batch_first=True, bidirectional=bidirectional, dropout=dropout if n_layers>1 else 0)\n",
    "        out_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "        if use_attn:\n",
    "            self.attn = AttentionPool(out_dim)\n",
    "            self.head = nn.Sequential(nn.Linear(out_dim, 256), nn.ReLU(), nn.Dropout(0.3), nn.Linear(256,1))\n",
    "        else:\n",
    "            self.head = nn.Sequential(nn.Linear(out_dim, 256), nn.ReLU(), nn.Dropout(0.3), nn.Linear(256,1))\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # x: [B, T, feat]\n",
    "        packed, _, _, _ = torch.nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        out_packed, (hn, cn) = self.lstm(packed)\n",
    "        out, _ = torch.nn.utils.rnn.pad_packed_sequence(out_packed, batch_first=True)  # [B, T, H]\n",
    "        if self.use_attn:\n",
    "            pooled, att_weights = self.attn(out, lengths)\n",
    "            logits = self.head(pooled).squeeze(1)\n",
    "            return logits, att_weights\n",
    "        else:\n",
    "            # use last valid timestep per example\n",
    "            idx = (lengths - 1).unsqueeze(1).unsqueeze(2).expand(-1, -1, out.size(2))\n",
    "            last = out.gather(1, idx).squeeze(1)\n",
    "            logits = self.head(last).squeeze(1)\n",
    "            return logits, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0081e209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build one dataset to read feat_dim\n",
    "train_ds = VideoEmbeddingDataset(\"train\")\n",
    "if len(train_ds) == 0:\n",
    "    raise RuntimeError(\"No train embeddings found. Run extract_embeddings first.\")\n",
    "sample_emb = np.load(train_ds.items[0])\n",
    "FEAT_DIM = int(sample_emb.shape[1])\n",
    "print(\"Feat dim:\", FEAT_DIM, \"Train videos:\", len(train_ds))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS,\n",
    "                          collate_fn=collate_fn, pin_memory=torch.cuda.is_available())\n",
    "val_loader = DataLoader(VideoEmbeddingDataset(\"val\"), batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS,\n",
    "                        collate_fn=collate_fn, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "model = TemporalModel(feat_dim=FEAT_DIM).to(DEVICE)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=2)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "scaler = GradScaler()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f119de",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_auc = 0.0\n",
    "start_epoch = 0\n",
    "last_ckpt = CHECKPOINT_DIR / \"temporal_last.pth\"\n",
    "if last_ckpt.exists():\n",
    "    ck = torch.load(last_ckpt, map_location=DEVICE)\n",
    "    model.load_state_dict(ck[\"model_state\"])\n",
    "    optimizer.load_state_dict(ck[\"optimizer_state\"])\n",
    "    start_epoch = ck.get(\"epoch\", 0) + 1\n",
    "    best_val_auc = ck.get(\"best_val_auc\", 0.0)\n",
    "    print(\"Resumed temporal from\", start_epoch, \"best\", best_val_auc)\n",
    "\n",
    "for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    all_preds, all_labels = [], []\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (seqs, lengths, labels, stems) in enumerate(tqdm(train_loader)):\n",
    "        seqs = seqs.to(DEVICE)\n",
    "        lengths = lengths.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(enabled=(DEVICE.type==\"cuda\")):\n",
    "            logits, _ = model(seqs, lengths)\n",
    "            loss = criterion(logits, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * seqs.size(0)\n",
    "        all_preds.append(torch.sigmoid(logits).detach().cpu())\n",
    "        all_labels.append(labels.detach().cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    train_auc = roc_auc_score(all_labels, all_preds)\n",
    "    train_loss = running_loss / len(train_ds)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for seqs, lengths, labels, stems in tqdm(val_loader):\n",
    "            seqs = seqs.to(DEVICE); lengths = lengths.to(DEVICE); labels = labels.to(DEVICE)\n",
    "            with autocast(enabled=(DEVICE.type==\"cuda\")):\n",
    "                logits, _ = model(seqs, lengths)\n",
    "                loss = criterion(logits, labels)\n",
    "            val_loss += loss.item() * seqs.size(0)\n",
    "            val_preds.append(torch.sigmoid(logits).cpu())\n",
    "            val_labels.append(labels.cpu())\n",
    "\n",
    "    val_preds = torch.cat(val_preds).numpy()\n",
    "    val_labels = torch.cat(val_labels).numpy()\n",
    "    val_auc = roc_auc_score(val_labels, val_preds)\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "\n",
    "    scheduler.step(val_auc)\n",
    "\n",
    "    ck = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"best_val_auc\": best_val_auc,\n",
    "        \"val_auc\": val_auc\n",
    "    }\n",
    "    torch.save(ck, last_ckpt)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        torch.save(ck, CHECKPOINT_DIR / \"temporal_best.pth\")\n",
    "        print(\"Saved new best temporal model\", best_val_auc)\n",
    "\n",
    "    print(f\"Epoch {epoch} done. train_loss={train_loss:.4f} train_auc={train_auc:.4f} val_loss={val_loss:.4f} val_auc={val_auc:.4f} time={(time.time()-t0):.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564d9317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training: evaluate on test split (video-level)\n",
    "best = CHECKPOINT_DIR / \"temporal_best.pth\"\n",
    "if best.exists():\n",
    "    ck = torch.load(best, map_location=DEVICE)\n",
    "    model.load_state_dict(ck[\"model_state\"])\n",
    "    print(\"Loaded best temporal model with val_auc:\", ck.get(\"best_val_auc\"))\n",
    "    # test dataset and loader\n",
    "    test_loader = DataLoader(VideoEmbeddingDataset(\"test\"), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "    model.eval()\n",
    "    t_preds, t_labels, stems_all = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for seqs, lengths, labels, stems in test_loader:\n",
    "            seqs = seqs.to(DEVICE); lengths = lengths.to(DEVICE)\n",
    "            with autocast(enabled=(DEVICE.type==\"cuda\")):\n",
    "                logits, _ = model(seqs, lengths)\n",
    "            t_preds.append(torch.sigmoid(logits).cpu())\n",
    "            t_labels.append(labels)\n",
    "            stems_all.extend(stems)\n",
    "    t_preds = torch.cat(t_preds).numpy()\n",
    "    t_labels = torch.cat(t_labels).numpy()\n",
    "    print(\"Test AUC:\", roc_auc_score(t_labels, t_preds))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
