{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "300a049e",
   "metadata": {},
   "source": [
    "# 06 - Train temporal model (LSTM + attention) on embeddings\n",
    "Trains a video-level LSTM aggregator using per-frame embeddings produced by the spatial model.\n",
    "Saves checkpoints: checkpoints/temporal/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358625b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Emb root: c:\\Users\\lkmah\\OneDrive\\Desktop\\Lokesh\\VS Code\\DeepFake_Detection_SIC\\embeddings\n",
      "Checkpoint dir: c:\\Users\\lkmah\\OneDrive\\Desktop\\Lokesh\\VS Code\\DeepFake_Detection_SIC\\checkpoints\\temporal\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json, time\n",
    "import random\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#from torch.cuda.amp import autocast, GradScaler\n",
    "# AMP disabled for temporal model (FP32 is more stable)\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# ------------- USER CONFIG -------------\n",
    "ROOT = Path.cwd().parent\n",
    "EMB_ROOT = ROOT / \"embeddings\"             # embeddings/<split>/<video_stem>.npy\n",
    "LABELS_JSON = ROOT / \"data\" / \"labels.json\"\n",
    "CHECKPOINT_DIR = ROOT / \"checkpoints\" / \"temporal\"\n",
    "NUM_EPOCHS = 25\n",
    "BATCH_SIZE = 16            # number of videos per batch\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NUM_WORKERS = 0            # keep 0 in notebooks; increase on robust machines\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PRINT_FREQ = 20\n",
    "LSTM_HIDDEN = 512\n",
    "LSTM_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "ATTENTION = True           # use attention pooling over LSTM outputs\n",
    "BIDIRECTIONAL = True\n",
    "# ---------------------------------------\n",
    "\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Device:\", DEVICE)\n",
    "print(\"Emb root:\", EMB_ROOT)\n",
    "print(\"Checkpoint dir:\", CHECKPOINT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707f5c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "with open(LABELS_JSON, \"r\") as f:\n",
    "    labels_map = json.load(f)\n",
    "\n",
    "def get_label_from_stem(stem):\n",
    "    if stem in labels_map:\n",
    "        return int(labels_map[stem])\n",
    "    for k,v in labels_map.items():\n",
    "        if stem in k:\n",
    "            return int(v)\n",
    "    raise KeyError(f\"Label for {stem} not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc43a609",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoEmbeddingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads one .npy per video:\n",
    "    shape = [T, feat_dim]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split):\n",
    "        self.root = EMB_ROOT / split\n",
    "        self.items = sorted(self.root.glob(\"*.npy\"))\n",
    "        self.items = [p for p in self.items if np.load(p).shape[0] > 0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.items[idx]\n",
    "        arr = np.load(p).astype(np.float32)\n",
    "        emb = torch.from_numpy(arr)\n",
    "        label = get_label_from_stem(p.stem)\n",
    "        return emb, torch.tensor(label, dtype=torch.float32), p.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47d80bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: list of (emb [T,feat], label, stem)\n",
    "    Pads sequences to longest T in batch (simple zero padding).\n",
    "    Returns tensors: seqs [B, Tmax, feat], lengths [B], labels [B]\n",
    "    \"\"\"\n",
    "    seqs, labels, stems = zip(*batch)\n",
    "    lengths = [s.shape[0] for s in seqs]\n",
    "    maxlen = max(lengths)\n",
    "    feat_dim = seqs[0].shape[1]\n",
    "    out = torch.zeros(len(seqs), maxlen, feat_dim, dtype=torch.float32)\n",
    "    for i, s in enumerate(seqs):\n",
    "        out[i, :s.shape[0], :] = s\n",
    "    labels = torch.stack(labels)\n",
    "    return out, torch.tensor(lengths, dtype=torch.long), labels, list(stems)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4e061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPool(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.att = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, h, lengths):\n",
    "        B, T, _ = h.shape\n",
    "\n",
    "        # never allow zero length\n",
    "        lengths = torch.clamp(lengths, min=1)\n",
    "\n",
    "        scores = self.att(h).squeeze(-1)  # [B, T]\n",
    "\n",
    "        mask = torch.arange(T, device=h.device).unsqueeze(0) >= lengths.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask, -1e9)\n",
    "\n",
    "        weights = torch.softmax(scores, dim=1)\n",
    "\n",
    "        # ABSOLUTE safety net\n",
    "        weights = torch.nan_to_num(weights, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        out = (h * weights.unsqueeze(-1)).sum(dim=1)\n",
    "        return out, weights\n",
    "\n",
    "class TemporalModel(nn.Module):\n",
    "    def __init__(self, feat_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            feat_dim, LSTM_HIDDEN, LSTM_LAYERS,\n",
    "            batch_first=True,\n",
    "            bidirectional=BIDIRECTIONAL,\n",
    "            dropout=DROPOUT if LSTM_LAYERS > 1 else 0\n",
    "        )\n",
    "        out_dim = LSTM_HIDDEN * (2 if BIDIRECTIONAL else 1)\n",
    "        self.attn = AttentionPool(out_dim)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(out_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # x: [B, T, feat], lengths: [B] (LongTensor)\n",
    "        # Handle case all lengths equal -> no need to pack (but pack still works).\n",
    "        if lengths.numel() == 0:\n",
    "            raise ValueError(\"Empty lengths tensor in TemporalModel.forward\")\n",
    "\n",
    "        # sort by lengths (descending)\n",
    "        lengths_sorted, perm_idx = lengths.sort(descending=True)\n",
    "        x_sorted = x[perm_idx]\n",
    "\n",
    "        # pack (pack expects CPU lengths)\n",
    "        packed = rnn_utils.pack_padded_sequence(x_sorted, lengths_sorted.cpu(), batch_first=True, enforce_sorted=True)\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        out_unpacked, _ = rnn_utils.pad_packed_sequence(packed_out, batch_first=True)  # [B_sorted, Tmax_sorted, H_out]\n",
    "\n",
    "        # unsort back to original order\n",
    "        _, unperm_idx = perm_idx.sort()\n",
    "        out = out_unpacked[unperm_idx]\n",
    "        lengths = lengths[unperm_idx]\n",
    "\n",
    "        pooled, att_weights = self.attn(out, lengths)\n",
    "        logits = self.head(pooled).squeeze(1)\n",
    "\n",
    "        # restore att_weights to original order too\n",
    "        if att_weights is not None:\n",
    "            att_weights = att_weights[unperm_idx]\n",
    "\n",
    "        return logits, att_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0ff4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_auc(y_true, y_pred):\n",
    "    if np.isnan(y_pred).any():\n",
    "        print(\"⚠ NaNs detected in predictions — skipping AUC\")\n",
    "        return float(\"nan\")\n",
    "    try:\n",
    "        return roc_auc_score(y_true, y_pred)\n",
    "    except Exception as e:\n",
    "        print(\"AUC error:\", e)\n",
    "        return float(\"nan\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78802354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, fname):\n",
    "    # copy model weights to CPU to reduce CUDA memory pressure and make file portable\n",
    "    cpu_state = state.copy()\n",
    "    cpu_state[\"model_state\"] = {k: v.cpu() for k, v in state[\"model_state\"].items()}\n",
    "    # optimizer state may contain tensors — move them to CPU as well (if present)\n",
    "    if \"optimizer_state\" in state and state[\"optimizer_state\"] is not None:\n",
    "        opt_state = state[\"optimizer_state\"]\n",
    "        # shallow copy\n",
    "        cpu_opt_state = {}\n",
    "        cpu_opt_state['state'] = {}\n",
    "        cpu_opt_state['param_groups'] = opt_state.get('param_groups', [])\n",
    "        for k, v in opt_state.get('state', {}).items():\n",
    "            cpu_opt_state['state'][k] = {sk: sv.cpu() if isinstance(sv, torch.Tensor) else sv\n",
    "                                         for sk, sv in v.items()}\n",
    "        cpu_state[\"optimizer_state\"] = cpu_opt_state\n",
    "    torch.save(cpu_state, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bd4f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feat dim: 1536 Train videos: 4066\n",
      "TemporalModel(\n",
      "  (lstm): LSTM(1536, 512, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (attn): AttentionPool(\n",
      "    (att): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Build one dataset to read feat_dim\n",
    "train_ds = VideoEmbeddingDataset(\"train\")\n",
    "val_ds = VideoEmbeddingDataset(\"val\")\n",
    "# sanity check \n",
    "if len(train_ds) == 0:\n",
    "    raise RuntimeError(\"No train embeddings found. Run extract_embeddings first.\")\n",
    "\n",
    "sample_emb = np.load(train_ds.items[0])\n",
    "FEAT_DIM = int(sample_emb.shape[1])\n",
    "print(\"Feat dim:\", FEAT_DIM, \"Train videos:\", len(train_ds))\n",
    "\n",
    "pin_memory = torch.cuda.is_available()\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS,\n",
    "                        collate_fn=collate_fn, pin_memory=pin_memory, persistent_workers=(NUM_WORKERS > 0))\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS,\n",
    "                        collate_fn=collate_fn, pin_memory=pin_memory, persistent_workers=(NUM_WORKERS > 0))\n",
    "\n",
    "model = TemporalModel(feat_dim=FEAT_DIM).to(DEVICE)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=2)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3437c508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  30%|██▉       | 76/255 [00:01<00:02, 61.53batch/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "NaN/Inf in embedding: c:\\Users\\lkmah\\OneDrive\\Desktop\\Lokesh\\VS Code\\DeepFake_Detection_SIC\\embeddings\\train\\837.npy",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 19\u001b[39m\n",
      "\u001b[32m     16\u001b[39m all_preds, all_labels = [], []\n",
      "\u001b[32m     17\u001b[39m running_loss = \u001b[32m0.0\u001b[39m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstems\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbatch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lkmah\\AppData\\Local\\Programs\\anaconda3\\envs\\aiml\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n",
      "\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n",
      "\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n",
      "\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lkmah\\AppData\\Local\\Programs\\anaconda3\\envs\\aiml\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n",
      "\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n",
      "\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n",
      "\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n",
      "\u001b[32m    738\u001b[39m ):\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lkmah\\AppData\\Local\\Programs\\anaconda3\\envs\\aiml\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n",
      "\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n",
      "\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lkmah\\AppData\\Local\\Programs\\anaconda3\\envs\\aiml\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n",
      "\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n",
      "\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lkmah\\AppData\\Local\\Programs\\anaconda3\\envs\\aiml\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n",
      "\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n",
      "\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mVideoEmbeddingDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n",
      "\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEmpty embedding detected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.isnan(arr).any() \u001b[38;5;129;01mor\u001b[39;00m np.isinf(arr).any():\n",
      "\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNaN/Inf in embedding: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m     24\u001b[39m emb = torch.from_numpy(arr.astype(np.float32))\n",
      "\u001b[32m     25\u001b[39m label = get_label_from_stem(stem)\n",
      "\n",
      "\u001b[31mValueError\u001b[39m: NaN/Inf in embedding: c:\\Users\\lkmah\\OneDrive\\Desktop\\Lokesh\\VS Code\\DeepFake_Detection_SIC\\embeddings\\train\\837.npy"
     ]
    }
   ],
   "source": [
    "best_val_auc = 0.0\n",
    "start_epoch = 0\n",
    "last_ckpt = CHECKPOINT_DIR / \"temporal_last.pth\"\n",
    "if last_ckpt.exists():\n",
    "    ck = torch.load(last_ckpt, map_location=DEVICE)\n",
    "    model.load_state_dict(ck[\"model_state\"])\n",
    "    optimizer.load_state_dict(ck[\"optimizer_state\"])\n",
    "    start_epoch = ck.get(\"epoch\", 0) + 1\n",
    "    best_val_auc = ck.get(\"best_val_auc\", 0.0)\n",
    "    print(\"Resumed temporal from\", start_epoch, \"best\", best_val_auc)\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    all_preds, all_labels = [], []\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for seqs, lengths, labels, stems in tqdm(train_loader, desc=f\"Epoch {epoch}\", unit=\"batch\", disable=False):\n",
    "        seqs = seqs.to(DEVICE)\n",
    "        lengths = lengths.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(seqs, lengths)\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # (optional but STRONGLY recommended)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * seqs.size(0)\n",
    "        all_preds.append(torch.sigmoid(logits).detach().cpu())\n",
    "        all_labels.append(labels.detach().cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "    print(\"NaNs in preds:\", np.isnan(all_preds).any())\n",
    "    print(\"NaNs in labels:\", np.isnan(all_labels).any())\n",
    "    print(\"Pred min/max:\", np.nanmin(all_preds), np.nanmax(all_preds))\n",
    "    train_auc = safe_auc(all_labels, all_preds)\n",
    "    #train_auc = roc_auc_score(all_labels, all_preds)\n",
    "    train_loss = running_loss / len(train_ds)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for seqs, lengths, labels, stems in tqdm(val_loader):\n",
    "            seqs = seqs.to(DEVICE); lengths = lengths.to(DEVICE); labels = labels.to(DEVICE)\n",
    "\n",
    "            logits, _ = model(seqs, lengths)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item() * seqs.size(0)\n",
    "            val_preds.append(torch.sigmoid(logits).cpu())\n",
    "            val_labels.append(labels.cpu())\n",
    "\n",
    "    val_preds = torch.cat(val_preds).numpy()\n",
    "    val_labels = torch.cat(val_labels).numpy()\n",
    "    val_auc = safe_auc(val_labels, val_preds)\n",
    "    #val_auc = roc_auc_score(val_labels, val_preds)\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "\n",
    "    scheduler.step(val_auc)\n",
    "\n",
    "    ck = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"best_val_auc\": best_val_auc,\n",
    "        \"val_auc\": val_auc,\n",
    "        \"scheduler_state\": scheduler.state_dict()  # optional\n",
    "    }\n",
    "\n",
    "    last_path = CHECKPOINT_DIR / \"temporal_last.pth\"\n",
    "    save_checkpoint(ck, last_path)\n",
    "\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        best_path = CHECKPOINT_DIR / \"temporal_best_valAUC.pth\"\n",
    "        save_checkpoint(ck, best_path)\n",
    "        print(f\"Saved new best model at epoch {epoch} val_auc={val_auc:.4f}\")\n",
    "\n",
    "    # also save epoch checkpoint (optional)\n",
    "    epoch_path = CHECKPOINT_DIR / f\"temporal_epoch_{epoch}.pth\"\n",
    "    save_checkpoint(ck, epoch_path)\n",
    "\n",
    "    print(f\"Epoch {epoch} done. train_loss={train_loss:.4f} train_auc={train_auc:.4f} val_loss={val_loss:.4f} val_auc={val_auc:.4f} time={(time.time()-t0):.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe45c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training: evaluate on test split (video-level)\n",
    "best = CHECKPOINT_DIR / \"temporal_best.pth\"\n",
    "if best.exists():\n",
    "    ck = torch.load(best, map_location=DEVICE)\n",
    "    model.load_state_dict(ck[\"model_state\"])\n",
    "    print(\"Loaded best temporal model with val_auc:\", ck.get(\"best_val_auc\"))\n",
    "    # test dataset and loader\n",
    "    test_loader = DataLoader(VideoEmbeddingDataset(\"test\"), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "    model.eval()\n",
    "    t_preds, t_labels, stems_all = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for seqs, lengths, labels, stems in test_loader:\n",
    "            seqs = seqs.to(DEVICE); lengths = lengths.to(DEVICE)\n",
    "            \n",
    "            logits, _ = model(seqs, lengths)\n",
    "            t_preds.append(torch.sigmoid(logits).cpu())\n",
    "            t_labels.append(labels)\n",
    "            stems_all.extend(stems)\n",
    "    t_preds = torch.cat(t_preds).numpy()\n",
    "    t_labels = torch.cat(t_labels).numpy()\n",
    "    print(\"Test AUC:\", roc_auc_score(t_labels, t_preds))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
